{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0341e8e6-dee4-4e82-97fd-dcf2aa103503",
   "metadata": {},
   "source": [
    "# Introduction to High Performance Computing\n",
    "\n",
    "\n",
    "**Courses**\n",
    "  + High-Performance Computing Technologies Course: <https://www.hpc.temple.edu/mhpc/hpc-technology/>\n",
    "  + What to do aout parallel prograaming: <https://news.ycombinator.com/item?id=36318280>\n",
    "  + <https://epcced.github.io/hpc-intro/>\n",
    "  + <https://www.ipht.fr/Pisp/gregoire.misguich/pp.php>\n",
    "  + <https://hpc.llnl.gov/documentation/tutorials>\n",
    "\n",
    "**Resources**\n",
    "  + <https://github.com/trevor-vincent/awesome-high-performance-computing>\n",
    "  + <https://viralinstruction.com/posts/hardware/>\n",
    "  + <https://www.archer.ac.uk/training/online/index.php#IntroHPC>\n",
    "  + <https://www.archer.ac.uk/training/courses/index.php#hands_on_intro>\n",
    "  + <https://www.archer.ac.uk/training/online/driving_test.php>\n",
    "  + https://www.sdsc.edu/services/service_rates_summary.html\n",
    "\n",
    "**Containers in HPC** \n",
    "  + <https://www.youtube.com/watch?v=PwI0tJHJOlo>\n",
    "  + <https://www.youtube.com/watch?v=WQTrA4-9ZXk>\n",
    "  + <https://apptainer.org/docs/user/main/index.html>\n",
    "  + https://apptainer.org/docs/user/main/docker_and_oci.html\n",
    "\n",
    "**Debuggers**\n",
    "-   <http://www.cs.uoregon.edu/research/tau/home.php>\n",
    "-   <https://vampir.eu/>\n",
    "\n",
    "**EPCC PRACE Training**\n",
    "-   <https://www.archer2.ac.uk/training/#upcoming-training>\n",
    "-   <https://www.archer2.ac.uk/training/courses/211202-package-users/#materials>\n",
    "-   <https://www.quia.com/quiz/8151816.html>\n",
    "-   <https://www.archer2.ac.uk/training/courses/210000-openmp-self-service/>\n",
    "-   <https://www.archer2.ac.uk/training/courses/210000-mpi-self-service/>\n",
    "-   <https://www.archer2.ac.uk/training/materials/>\n",
    "-   Youtube video list: <https://www.youtube.com/watch?v=_h55hwpLwoE&list=PLD0xgZGaUd1IV8VgXb1ggOLkEv19JmZiP>\n",
    "\n",
    "Reference Materials\n",
    "See presentations `hpc-intro.pdf`, `07-HPC-Distributed.pdf` and\n",
    "`07-Parallel-general-metrics.pdf` . Special care to metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101dcd1",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "There is a point where a serial version of our code is not the most\n",
    "optimal way to exploit our computational resources (but it might be in\n",
    "the case of embarrassingly parallel problems where you can just run\n",
    "several programs at once). For instance, you might want to use all the\n",
    "cores on your multicore system, ideally reducing the execution time, or\n",
    "you need to explore larger system sizes that could consume a lot of\n",
    "memory or need too much time.\n",
    "\n",
    "Typically, [Moore's\n",
    "law](https://en.wikipedia.org/wiki/Moore%27s_law?useskin=vector) allowed\n",
    "to wait for a bit in order to get a better machine so your algorithms\n",
    "will run faster.\n",
    "\n",
    "<https://ourworldindata.org/uploads/2020/11/Transistor-Count-over-time.png>\n",
    "\n",
    "But due to physics limitation and power considerations, it is now\n",
    "typical to have\n",
    "[multicore](https://en.wikipedia.org/wiki/Multi-core_processor?useskin=vector)\n",
    "systems\n",
    "\n",
    "<https://www.youtube.com/watch?v=Qlv5pB6u534> (The physics ending moore laws)\n",
    "\n",
    "\n",
    "<https://i.stack.imgur.com/fRJgk.png>\n",
    "\n",
    "Recently, power considerations are being more and more relevant:\n",
    "\n",
    "-   <https://en.wikipedia.org/wiki/Performance_per_watt?useskin=vector>\n",
    "\n",
    "-   <https://en.wikipedia.org/wiki/Koomey%27s_law?useskin=vector>\n",
    "\n",
    "-   <https://www.apple.com/newsroom/2022/03/apple-unveils-m1-ultra-the-worlds-most-powerful-chip-for-a-personal-computer/>\n",
    "\n",
    "-   <https://www.extremetech.com/extreme/328541-the-apple-m1-pro-and-m1-maxs-power-efficiency-should-rattle-intel-amd>)\n",
    "\n",
    "    <https://i.extremetech.com/imagery/content-types/07CyoCCWMzGjurj8zpuiYO4/images-2.jpg>\n",
    "\n",
    "At the same time, the computational problems size and/or complexity has\n",
    "been steadily increasing in time, requiring [distributed\n",
    "computing](https://en.wikipedia.org/wiki/Distributed_computing?useskin=vector)\n",
    "techniques.\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Distributed-parallel.svg/600px-Distributed-parallel.svg.png>\n",
    "\n",
    "(see also\n",
    "<https://en.wikipedia.org/wiki/Flynn%27s_taxonomy?useskin=vector>).\n",
    "Recently, besides CPU parallelization, the GPU parallelization has\n",
    "become very relevant (see\n",
    "<https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units?useskin=vector>\n",
    "), where [CUDA](https://en.wikipedia.org/wiki/CUDA?useskin=vector) ,\n",
    "[OpenACC](https://en.wikipedia.org/wiki/OpenACC?useskin=vector), and\n",
    "others, are the relevant technologies.\n",
    "\n",
    "In our case, we will be more focused on the [cluster\n",
    "computing](https://en.wikipedia.org/w/index.php?title=Computer_cluster&useskin=vector)\n",
    "aspect, while there are more\n",
    "[HPC](https://en.wikipedia.org/wiki/High-performance_computing?useskin=vector)\n",
    "approaches, like [grid\n",
    "computing](https://en.wikipedia.org/wiki/Grid_computing?useskin=vector),\n",
    "[cloud\n",
    "computing](https://en.wikipedia.org/wiki/Cloud_computing?useskin=vector),\n",
    "and so on. One of the goals of\n",
    "[HPC](https://en.wikipedia.org/wiki/High-performance_computing?useskin=vector)\n",
    "is to get better results faster and/or to exploit better current or\n",
    "future resources.\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/IBM_Blue_Gene_P_supercomputer.jpg/600px-IBM_Blue_Gene_P_supercomputer.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f793e",
   "metadata": {},
   "source": [
    "\n",
    "## Basics of parallel metrics\n",
    "\n",
    "But, as usual, you should always measure. All programs have a serial\n",
    "part that cannot be parallelized and a parallel part than can. Using\n",
    "more processors/threads can reduce only the parallel, so a 100% serial\n",
    "program cannot really take advantage of a parallel system. This is known\n",
    "as Amdahls law,\n",
    "<https://en.wikipedia.org/wiki/Amdahl%27s_law?useskin=vector>\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/e/ea/AmdahlsLaw.svg>\n",
    "\n",
    "At the end, the user must also gauge its application performance.\n",
    "Blindly reserve of HPC resources represent a non efficient cluster use,\n",
    "and higher costs. In this regard, parallel metrics are really crucial.\n",
    "The next to figures show the speedup and the parallel efficiency. As you\n",
    "can see, they are limited by the hardware (and algorithms)\n",
    "\n",
    "- Speedup: \n",
    "  <img src=\"speedup.png\" class=centerimg50>\n",
    "- Parallel efficiency: \n",
    "  <img src=\"efficiency.png\" class=centerimg50>\n",
    "\n",
    "\n",
    "Rooftop model:\n",
    "- <https://en.wikipedia.org/wiki/Roofline_model?useskin=vector>\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5b599",
   "metadata": {},
   "source": [
    "\n",
    "## Practical overview of a cluster resources and use\n",
    "\n",
    "There are many aspects to take into account in the HPC field. If you are\n",
    "a user, you should know abount the type of parallelization (shared\n",
    "memory, disitributed memory, gpu programming), the type of hardware you\n",
    "are using, the resource manager, the data storage and so on. The goal of\n",
    "a system administrator is to make that easier, but that is not always\n",
    "possible. Check the `12-12-hkhlr_quick_reference-goethe-hlr` (from\n",
    "<https://csc.uni-frankfurt.de/wiki/doku.php?id=public:start>) for an\n",
    "example of a typical cluster config and offerings.\n",
    "\n",
    "These are examples from the Archer cluster at\n",
    "<https://www.archer2.ac.uk/>\n",
    "\n",
    "<img src=\"Archer1.png\" style=\"width:80.0%\"\n",
    "data-align=\"center\" />\n",
    "\n",
    "<img src=\"Archer2.png\" class=centerimg50/>\n",
    "\n",
    "<img src=\"Archer3.png\" class=centerimg50/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee4fc",
   "metadata": {},
   "source": [
    "\n",
    "In the following, we will see some basic examples for HPC, such us\n",
    "\n",
    "-   Shared memory: with openmp\n",
    "-   Distributed memory: using mpi\n",
    "-   Multiple processes: using gnu parallel\n",
    "-   C++ threads\n",
    "-   TODO C++ parallel algorithms\n",
    "\n",
    "Video: Introduction to shared and distributed memory: <https://math-gpt.org/video/hls_4e3c1ffe-ee8e-49fb-a62d-a10615c0483d>\n",
    "\n",
    "Video: Introduction to gpu programming with cuda: <https://math-gpt.org/video/hls_a9482f1a-7013-423e-ae0e-d4fd83edf997>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc947e6-d4aa-4844-9336-48d725c7ac8f",
   "metadata": {},
   "source": [
    "## Openmp, shared memory\n",
    "\n",
    "In the share memory paradigm, threads (lighweight processes without their own context) share or can \"see\" the same memory:\n",
    "\n",
    "\n",
    "<img src=\"https://hpc-tutorials.llnl.gov/openmp/images/uma.gif\" width=30%> <img src=\"https://hpc-tutorials.llnl.gov/openmp/images/numa.gif\" width=30%>\n",
    " \n",
    " \n",
    "\n",
    "\n",
    "Here, a main process creates/destriys threads as needed\n",
    "<img src=\"https://hpc-tutorials.llnl.gov/openmp/images/fork_join.gif\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25f923-c974-439e-956f-066c98c6435f",
   "metadata": {},
   "source": [
    "The following code shows a very simple parallelization using openmp,\n",
    "which allows tu share memory and run on several threads.\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  std::cout << \"BEFORE\\n\";\n",
    "#pragma omp parallel\n",
    "  {\n",
    "    std::cout << \"Hola mundo\\n\";\n",
    "  }\n",
    "  std::cout << \"AFTER\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "To activate multi-threading, compile it as\n",
    "\n",
    "``` bash\n",
    "g++ -fopenmp codes/openmp.cpp\n",
    "```\n",
    "\n",
    "To run it, you can control the number of threads using the environment\n",
    "variable `OMP_NUM_THREADS`:\n",
    "\n",
    "``` bash\n",
    "g++ -fopenmp codes/openmp.cpp\n",
    "echo \"Running with 2 threads\"\n",
    "OMP_NUM_THREADS=2 ./a.out\n",
    "echo \"Running with 4 threads\"\n",
    "OMP_NUM_THREADS=4 ./a.out\n",
    "```\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1.  Modify the previous exercise to identify the thread which is\n",
    "    printing. Find a function to get the \"thread id\".\n",
    "\n",
    "2.  Besides the thread id, print the number of threads and the hostname.\n",
    "    Print the number of threads outside the parallel region. Does that\n",
    "    make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f9d954-9d69-4b9f-b614-5772ccf48ab0",
   "metadata": {},
   "source": [
    "## MPI, distributed memory\n",
    "\n",
    "MPI, the Message Passing Interface, is a library API that allows process\n",
    "to interchange data in a distributed memory context. It is more comple\n",
    "that openmp, but also opens the door to a greater scale since we can use\n",
    "many computers, increasing both our computational power and memory\n",
    "capacity (if done correctly and efficiently). See: <https://www.mpi-forum.org/docs/>\n",
    "\n",
    "<img src=\"https://hpc-tutorials.llnl.gov/mpi/images/distributed_mem.gif\" alt=\"https://hpc-tutorials.llnl.gov/mpi/images/distributed_mem.gif\" width=\"50%\" align=\"center\">\n",
    "\n",
    "<img src=\"https://developer-blogs.nvidia.com/wp-content/uploads/2013/03/LaunchMPI1.png\" alt=\"https://developer-blogs.nvidia.com/wp-content/uploads/2013/03/LaunchMPI1.png\" width=\"50%\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab25ab-d62b-43c5-9f45-ce7db12ecd60",
   "metadata": {},
   "source": [
    "The following shows the basic structure of a MPI program. It creates\n",
    "several **processes** that can communicate with each other, and can be\n",
    "run in multiple machines (for an introduction, see:\n",
    "<https://mpitutorial.com/tutorials/mpi-introduction/>)\n",
    "\n",
    "``` c++\n",
    "#include <mpi.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    // Initialize the MPI environment\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    // Get the number of processes\n",
    "    int np;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "    // Get the rank of the process\n",
    "    int pid;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "\n",
    "    // Get the name of the processor\n",
    "    char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "    int name_len;\n",
    "    MPI_Get_processor_name(processor_name, &name_len);\n",
    "\n",
    "    // Print off a hello world message\n",
    "    printf(\"Hello world from processor %s, rank %d out of %d processes\\n\",\n",
    "           processor_name, pid, np);\n",
    "\n",
    "    // Finalize the MPI environment.\n",
    "    MPI_Finalize();\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "You can compile it as\n",
    "\n",
    "``` bash\n",
    "mpic++  mpi.cpp\n",
    "```\n",
    "\n",
    "(If you want to see all the flags, use `mpic++ --showme`)\n",
    "\n",
    "And now run it as\n",
    "\n",
    "``` bash\n",
    "mpirun -np 4 ./a.out\n",
    "```\n",
    "\n",
    "You can also specifiy different machines to run on, but you will need to\n",
    "have configured passwordless access to those machines. Or better, use `slurm` . \n",
    "\n",
    "### Exercises\n",
    "- Run on two nodes the same process\n",
    "- Interchange some message between the two nodes: use MPI_Send and MPI_Recv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebafac8-2ba9-4343-926d-85f1b564b44f",
   "metadata": {},
   "source": [
    "## Parallelism in python\n",
    "\n",
    "- <https://realpython.com/intro-to-python-threading/>\n",
    "- <https://www.dask.org/>\n",
    "- <https://github.com/pybind/pybind11>\n",
    "- <https://www.phoronix.com/news/NumPy-2.3-Released>\n",
    "\n",
    "Also :\n",
    "- <https://realpython.com/python312-subinterpreters/>\n",
    "- <https://docs.python.org/3/whatsnew/3.12.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56743449-cfe8-40ed-a630-53f0f0618249",
   "metadata": {},
   "source": [
    "## Threads from c++11\n",
    "\n",
    "The `c++11` standard included, among many other usefull things, the use\n",
    "a **thread**. A thread is a lightweight process that can be launched in\n",
    "parallel with other threads from a parent process. In the following we\n",
    "will see some very simple examples since at the end we will focus mainly\n",
    "on OpenMP (where threads are the key and the memory is shared) and MPI\n",
    "(where processes are the basic unit and memory is distributed).\n",
    "\n",
    "The following example are based on\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/thread>\n",
    "-   <https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab6/>\n",
    "\n",
    "The following example shows how to create a thread from a given process,\n",
    "and its output:\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "\n",
    "void func(int x);\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    std::thread th1(&func, 100);\n",
    "    std::thread th2(&func, 200);\n",
    "    th1.join();\n",
    "    std::cout << \"Outside thread\" << std::endl;\n",
    "    th2.join();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void func(int x) {\n",
    "    std::cout << \"Inside thread \" << x << std::endl;\n",
    "    std::thread::id this_id = std::this_thread::get_id();\n",
    "    std::cout << \"This is thread_id: \" << this_id << std::endl;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile it as\n",
    "\n",
    "``` bash\n",
    "g++ -std=c++11 thread-v1.cpp\n",
    "```\n",
    "\n",
    "The folowwing is an example of the output:\n",
    "\n",
    "|                |        |                      |        |        |                      |                |\n",
    "|----------------|--------|----------------------|--------|--------|----------------------|----------------|\n",
    "| Inside         | thread | Inside               | thread | 100200 |                      |                |\n",
    "|                |        |                      |        |        |                      |                |\n",
    "| This           | is     | thread<sub>id</sub>: | This   | is     | thread<sub>id</sub>: | 0x700003c34000 |\n",
    "| 0x700003cb7000 |        |                      |        |        |                      |                |\n",
    "| Outside        | thread |                      |        |        |                      |                |\n",
    "\n",
    "Run it several times, you will obtain different outputs, many times they\n",
    "will be mangled. Why? because the threads are running in parallel and\n",
    "their output is not independent of each other, not synced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1da406-5166-4b1a-a668-9afa90f1546d",
   "metadata": {},
   "source": [
    "To check that we are really running two threads, let's increase the\n",
    "computational effort inside function `func` and then, while the program\n",
    "is running, use top or htop to check what is running on your computer.\n",
    "Notice that the cpu use percentage is around `200%`:\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "\n",
    "void func(double x, int nsecs);\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    const int secs = std::atoi(argv[1]);\n",
    "    std::thread th1(&func, 100, secs);\n",
    "    std::thread th2(&func, 200, secs);\n",
    "    std::thread th3(&func, 300, secs);\n",
    "    th1.join();\n",
    "    std::cout << \"Outside thread\" << std::endl;\n",
    "    th2.join();\n",
    "    th3.join();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void func(double x, int nsecs) {\n",
    "    std::cout << \"Inside thread \" << x << std::endl;\n",
    "    std::this_thread::sleep_for (std::chrono::seconds(nsecs)); // make this sleep, does not consume a lot of resources\n",
    "    for (int ii = 0; ii < 100000000; ++ii) {\n",
    "        x += std::fabs(x*std::sin(x) + std::sqrt(x)/3.4455)/(ii+1);\n",
    "    }\n",
    "    std::cout << \"Getting out of thread \" << x << std::endl;\n",
    "}\n",
    "```\n",
    "\n",
    "The following is an example of the output:\n",
    "\n",
    "|         |        |        |        |               |\n",
    "|---------|--------|--------|--------|---------------|\n",
    "| Inside  | thread | Inside | thread | 100           |\n",
    "| 200     |        |        |        |               |\n",
    "| Getting | out    | of     | thread | 9387820000.0  |\n",
    "| Outside | thread |        |        |               |\n",
    "| Getting | out    | of     | thread | 18849600000.0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35661f3a-b923-47b9-a396-d2955aee4bbd",
   "metadata": {},
   "source": [
    "To synchronize threads, you can use a mutex. This is useful in case you\n",
    "need to sort out the printing, or, more importantly, to syncrhonize a\n",
    "writing operation on some common variable. The following is a simple\n",
    "example:\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <chrono>\n",
    "#include <mutex>\n",
    "\n",
    "std::mutex g_display_mutex;\n",
    "\n",
    "void foo()\n",
    "{\n",
    "    std::thread::id this_id = std::this_thread::get_id();\n",
    "\n",
    "    g_display_mutex.lock();\n",
    "    std::cout << \"thread \" << this_id << \" sleeping...\\n\";\n",
    "    g_display_mutex.unlock();\n",
    "\n",
    "    std::this_thread::sleep_for(std::chrono::seconds(1));\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    std::thread t1(foo);\n",
    "    std::thread t2(foo);\n",
    "\n",
    "    t1.join();\n",
    "    t2.join();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "|        |                |           |\n",
    "|--------|----------------|-----------|\n",
    "| thread | 0x70000709a000 | sleeping… |\n",
    "| thread | 0x70000711d000 | sleeping… |\n",
    "\n",
    "Repeat several times. Although the thread id will change, the output\n",
    "will not be mangled.\n",
    "\n",
    "There is much more about threads, but since our focus will turn to\n",
    "OpenMP, we will stop here. For more info check\n",
    "<https://en.cppreference.com/w/cpp/thread/thread>\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fix the following code, which has a race condition, using a mutex (ref:\n",
    "<https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab6/>)\n",
    ":\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <thread>\n",
    "\n",
    "void square(const int x, int & result);\n",
    "\n",
    "int main() {\n",
    "    int accum = 0;\n",
    "    std::vector<std::thread> ths;\n",
    "    for (int i = 1; i <= 20; i++) {\n",
    "        ths.push_back(std::thread(&square, i, std::ref(accum)));\n",
    "    }\n",
    "\n",
    "    for (auto & th : ths) {\n",
    "        th.join();\n",
    "    }\n",
    "    std::cout << \"accum = \" << accum << std::endl;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void square(int x, int &result) {\n",
    "    result += x * x;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "``` bash\n",
    "accum = 2870\n",
    "```\n",
    "\n",
    "The correct answer is `2870`, but if you repeat the execution many times\n",
    "you will find different results. For instance, repeating the execution\n",
    "1000 times and checking for the unique answers one gets\n",
    "\n",
    "``` bash\n",
    "for i in {1..1000}; do ./a.out; done | sort | uniq -c\n",
    "```\n",
    "\n",
    "``` bash\n",
    "  2 accum = 2509\n",
    "  1 accum = 2674\n",
    "  2 accum = 2749\n",
    "  1 accum = 2806\n",
    "  1 accum = 2834\n",
    "  4 accum = 2845\n",
    "  1 accum = 2854\n",
    "  1 accum = 2861\n",
    "  2 accum = 2866\n",
    "  6 accum = 2869\n",
    "979 accum = 2870\n",
    "```\n",
    "\n",
    "which shows that it not always yield 2870 ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c859ec-40bb-4c0d-bbb8-ce0a3cebe352",
   "metadata": {},
   "source": [
    "## Parallel algorithms in c++\n",
    "\n",
    "Since `c++17`, it is possible to execute some stl algorithms in parallel\n",
    "(shared memory), without explictly using threads. See:\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t>\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm>\n",
    "-   <https://en.cppreference.com/w/cpp/experimental/parallelism>\n",
    "\n",
    "This a simple example for performing a sum\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "\n",
    "int main() {\n",
    "    const long ARRAY_SIZE = 100000000;\n",
    "    std::vector<double> myArray(ARRAY_SIZE);\n",
    "    std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "    // sequential execution\n",
    "    auto sum_seq = std::accumulate(myArray.begin(), myArray.end(), 0.0);\n",
    "    std::cout << \"Sequential sum: \" << sum_seq << std::endl;\n",
    "\n",
    "    // parallel execution\n",
    "    auto sum_par = std::reduce(std::execution::par, myArray.begin(), myArray.end());\n",
    "    std::cout << \"Parallel sum: \" << sum_par << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "To compile, use\n",
    "\n",
    "``` bash\n",
    "g++ -std=c++17 par.cpp -ltbb\n",
    "```\n",
    "\n",
    "This is linking with an intel threads implementation.\n",
    "\n",
    "You can of course measure how much time is spent on each part. To do so,\n",
    "we will use chrono:\n",
    "\n",
    "-   Implementation 1:\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "\n",
    "int main() {\n",
    "    const long ARRAY_SIZE = 100000000;\n",
    "    std::vector<double> myArray(ARRAY_SIZE);\n",
    "    std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "    // sequential execution\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "    auto sum_seq = std::accumulate(myArray.begin(), myArray.end(), 0.0);\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    auto seq_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n",
    "    std::cout << \"Sequential sum: \" << sum_seq << \"( took : \" << seq_duration.count()/1000.0 << \" s)\" << std::endl;\n",
    "\n",
    "    // parallel execution\n",
    "    start_time = std::chrono::high_resolution_clock::now();\n",
    "    auto sum_par = std::reduce(std::execution::par, myArray.begin(), myArray.end());\n",
    "    end_time = std::chrono::high_resolution_clock::now();\n",
    "    seq_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n",
    "    std::cout << \"Parallel sum: \" << sum_par << \"( took : \" << seq_duration.count()/1000.0 << \" s)\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "-   Implementation 2:\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "#include <chrono>\n",
    "\n",
    "template<typename Func>\n",
    "void time_function(Func func);\n",
    "\n",
    "int main() {\n",
    "  const long ARRAY_SIZE = 100000000;\n",
    "  std::vector<double> myArray(ARRAY_SIZE);\n",
    "  std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "  // sequential execution\n",
    "  auto serial = [&myArray](){return std::accumulate(myArray.begin(), myArray.end(), 0.0);};\n",
    "  time_function(serial);\n",
    "\n",
    "  // parallel execution\n",
    "  auto parallel = [&myArray](){return std::reduce(std::execution::par, myArray.begin(), myArray.end());};\n",
    "  time_function(parallel);\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "template<typename Func>\n",
    "void time_function(Func func) {\n",
    "  auto start = std::chrono::high_resolution_clock::now();\n",
    "  func();\n",
    "  auto end = std::chrono::high_resolution_clock::now();\n",
    "  auto duration_ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();\n",
    "  std::cout << \"Elapsed time: \" << duration_ms/1000.0 << \"  s\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The standard does not specify a way to control the nuber tof threads. If\n",
    "you want to do so, and you are using Intel Threads Block implementation,\n",
    "you can add gthe following header\n",
    "\n",
    "``` c++\n",
    "#include <tbb/task_scheduler_init.h>\n",
    "```\n",
    "\n",
    "and then , at some point, specify the thread total (in this case, 4)\n",
    "\n",
    "``` c++\n",
    "tbb::task_scheduler_init init(4);\n",
    "```\n",
    "\n",
    "To learn more about the parallel algs, check\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262565db-f2e9-486f-9c75-acfee8d3cba3",
   "metadata": {},
   "source": [
    "## Gpu Programming \n",
    "### CUDA: Nvidia\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/gpu-devotes-more-transistors-to-data-processing.png\" alt=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/_images/gpu-devotes-more-transistors-to-data-processing.png\" width=\"50%\" align=\"center\">\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1c9bc3b-8d6f-4d6a-b199-d52a7c0500db_1400x900.png\" alt=\"https://www.generativevalue.com/p/nvidia-past-present-and-future\" width=\"50%\" align=\"center\">\n",
    "\n",
    "\n",
    "### AMD: Rocm\n",
    "<https://rocm.docs.amd.com/projects/HIP/en/latest/understand/programming_model.html>\n",
    "\n",
    "<img src=\"https://cgmb-rocm-docs.readthedocs.io/en/latest/_images/ROCm_Stack.png\" alt=\"https://cgmb-rocm-docs.readthedocs.io/en/latest/_images/ROCm_Stack.png\" width=\"50%\" align=\"center\">\n",
    "\n",
    "<img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed59b592-486f-4579-9187-1ae7d7921f3c_1597x897.jpeg\" alt=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed59b592-486f-4579-9187-1ae7d7921f3c_1597x897.jpeg\" width=\"50%\" align=\"center\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Others\n",
    "OpenACC, OpenMP, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d21e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

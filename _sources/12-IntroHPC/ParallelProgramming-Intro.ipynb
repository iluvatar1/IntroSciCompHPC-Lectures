{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0341e8e6-dee4-4e82-97fd-dcf2aa103503",
   "metadata": {},
   "source": [
    "# Introduction to High Performance Computing\n",
    "\n",
    "\n",
    "**Courses**\n",
    "  + High-Performance Computing Technologies Course: <https://www.hpc.temple.edu/mhpc/hpc-technology/>\n",
    "  + What to do aout parallel prograaming: <https://news.ycombinator.com/item?id=36318280>\n",
    "  + <https://epcced.github.io/hpc-intro/>\n",
    "  + <https://www.ipht.fr/Pisp/gregoire.misguich/pp.php>\n",
    "  + <https://hpc.llnl.gov/documentation/tutorials>\n",
    "\n",
    "**Resources**\n",
    "  + <https://github.com/trevor-vincent/awesome-high-performance-computing>\n",
    "  + <https://viralinstruction.com/posts/hardware/>\n",
    "  + <https://www.archer.ac.uk/training/online/index.php#IntroHPC>\n",
    "  + <https://www.archer.ac.uk/training/courses/index.php#hands_on_intro>\n",
    "  + <https://www.archer.ac.uk/training/online/driving_test.php>\n",
    "  + https://www.sdsc.edu/services/service_rates_summary.html\n",
    "\n",
    "**Containers in HPC** \n",
    "  + <https://www.youtube.com/watch?v=PwI0tJHJOlo>\n",
    "  + <https://www.youtube.com/watch?v=WQTrA4-9ZXk>\n",
    "  + <https://apptainer.org/docs/user/main/index.html>\n",
    "  + https://apptainer.org/docs/user/main/docker_and_oci.html\n",
    "\n",
    "**Debuggers**\n",
    "-   <http://www.cs.uoregon.edu/research/tau/home.php>\n",
    "-   <https://vampir.eu/>\n",
    "\n",
    "**EPCC PRACE Training**\n",
    "-   <https://www.archer2.ac.uk/training/#upcoming-training>\n",
    "-   <https://www.archer2.ac.uk/training/courses/211202-package-users/#materials>\n",
    "-   <https://www.quia.com/quiz/8151816.html>\n",
    "-   <https://www.archer2.ac.uk/training/courses/210000-openmp-self-service/>\n",
    "-   <https://www.archer2.ac.uk/training/courses/210000-mpi-self-service/>\n",
    "-   <https://www.archer2.ac.uk/training/materials/>\n",
    "-   Youtube video list: <https://www.youtube.com/watch?v=_h55hwpLwoE&list=PLD0xgZGaUd1IV8VgXb1ggOLkEv19JmZiP>\n",
    "\n",
    "Reference Materials\n",
    "See presentations `hpc-intro.pdf`, `07-HPC-Distributed.pdf` and\n",
    "`07-Parallel-general-metrics.pdf` . Special care to metrics.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101dcd1",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "There is a point where a serial version of our code is not the most\n",
    "optimal way to exploit our computational resources (but it might be in\n",
    "the case of embarrassingly parallel problems where you can just run\n",
    "several programs at once). For instance, you might want to use all the\n",
    "cores on your multicore system, ideally reducing the execution time, or\n",
    "you need to explore larger system sizes that could consume a lot of\n",
    "memory or need too much time.\n",
    "\n",
    "Typically, [Moore's\n",
    "law](https://en.wikipedia.org/wiki/Moore%27s_law?useskin=vector) allowed\n",
    "to wait for a bit in order to get a better machine so your algorithms\n",
    "will run faster.\n",
    "\n",
    "<https://ourworldindata.org/uploads/2020/11/Transistor-Count-over-time.png>\n",
    "\n",
    "But due to physics limitation and power considerations, it is now\n",
    "typical to have\n",
    "[multicore](https://en.wikipedia.org/wiki/Multi-core_processor?useskin=vector)\n",
    "systems\n",
    "\n",
    "<https://www.youtube.com/watch?v=Qlv5pB6u534> (The physics ending moore laws)\n",
    "\n",
    "\n",
    "<https://i.stack.imgur.com/fRJgk.png>\n",
    "\n",
    "Recently, power considerations are being more and more relevant:\n",
    "\n",
    "-   <https://en.wikipedia.org/wiki/Performance_per_watt?useskin=vector>\n",
    "\n",
    "-   <https://en.wikipedia.org/wiki/Koomey%27s_law?useskin=vector>\n",
    "\n",
    "-   <https://www.apple.com/newsroom/2022/03/apple-unveils-m1-ultra-the-worlds-most-powerful-chip-for-a-personal-computer/>\n",
    "\n",
    "-   <https://www.extremetech.com/extreme/328541-the-apple-m1-pro-and-m1-maxs-power-efficiency-should-rattle-intel-amd>)\n",
    "\n",
    "    <https://i.extremetech.com/imagery/content-types/07CyoCCWMzGjurj8zpuiYO4/images-2.jpg>\n",
    "\n",
    "At the same time, the computational problems size and/or complexity has\n",
    "been steadily increasing in time, requiring [distributed\n",
    "computing](https://en.wikipedia.org/wiki/Distributed_computing?useskin=vector)\n",
    "techniques.\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Distributed-parallel.svg/600px-Distributed-parallel.svg.png>\n",
    "\n",
    "(see also\n",
    "<https://en.wikipedia.org/wiki/Flynn%27s_taxonomy?useskin=vector>).\n",
    "Recently, besides CPU parallelization, the GPU parallelization has\n",
    "become very relevant (see\n",
    "<https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units?useskin=vector>\n",
    "), where [CUDA](https://en.wikipedia.org/wiki/CUDA?useskin=vector) ,\n",
    "[OpenACC](https://en.wikipedia.org/wiki/OpenACC?useskin=vector), and\n",
    "others, are the relevant technologies.\n",
    "\n",
    "In our case, we will be more focused on the [cluster\n",
    "computing](https://en.wikipedia.org/w/index.php?title=Computer_cluster&useskin=vector)\n",
    "aspect, while there are more\n",
    "[HPC](https://en.wikipedia.org/wiki/High-performance_computing?useskin=vector)\n",
    "approaches, like [grid\n",
    "computing](https://en.wikipedia.org/wiki/Grid_computing?useskin=vector),\n",
    "[cloud\n",
    "computing](https://en.wikipedia.org/wiki/Cloud_computing?useskin=vector),\n",
    "and so on. One of the goals of\n",
    "[HPC](https://en.wikipedia.org/wiki/High-performance_computing?useskin=vector)\n",
    "is to get better results faster and/or to exploit better current or\n",
    "future resources.\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/thumb/d/d3/IBM_Blue_Gene_P_supercomputer.jpg/600px-IBM_Blue_Gene_P_supercomputer.jpg>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2f793e",
   "metadata": {},
   "source": [
    "\n",
    "## Basics of parallel metrics\n",
    "\n",
    "But, as usual, you should always measure. All programs have a serial\n",
    "part that cannot be parallelized and a parallel part than can. Using\n",
    "more processors/threads can reduce only the parallel, so a 100% serial\n",
    "program cannot really take advantage of a parallel system. This is known\n",
    "as Amdahls law,\n",
    "<https://en.wikipedia.org/wiki/Amdahl%27s_law?useskin=vector>\n",
    "\n",
    "<https://upload.wikimedia.org/wikipedia/commons/e/ea/AmdahlsLaw.svg>\n",
    "\n",
    "At the end, the user must also gauge its application performance.\n",
    "Blindly reserve of HPC resources represent a non efficient cluster use,\n",
    "and higher costs. In this regard, parallel metrics are really crucial.\n",
    "The next to figures show the speedup and the parallel efficiency. As you\n",
    "can see, they are limited by the hardware (and algorithms)\n",
    "\n",
    "- Speedup: \n",
    "  <img src=\"speedup.png\" class=centerimg50>\n",
    "- Parallel efficiency: \n",
    "  <img src=\"efficiency.png\" class=centerimg50>\n",
    "\n",
    "\n",
    "Rooftop model:\n",
    "- <https://en.wikipedia.org/wiki/Roofline_model?useskin=vector>\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5b599",
   "metadata": {},
   "source": [
    "\n",
    "## Practical overview of a cluster resources and use\n",
    "\n",
    "There are many aspects to take into account in the HPC field. If you are\n",
    "a user, you should know abount the type of parallelization (shared\n",
    "memory, disitributed memory, gpu programming), the type of hardware you\n",
    "are using, the resource manager, the data storage and so on. The goal of\n",
    "a system administrator is to make that easier, but that is not always\n",
    "possible. Check the `12-12-hkhlr_quick_reference-goethe-hlr` (from\n",
    "<https://csc.uni-frankfurt.de/wiki/doku.php?id=public:start>) for an\n",
    "example of a typical cluster config and offerings.\n",
    "\n",
    "These are examples from the Archer cluster at\n",
    "<https://www.archer2.ac.uk/>\n",
    "\n",
    "<img src=\"Archer1.png\" style=\"width:80.0%\"\n",
    "data-align=\"center\" />\n",
    "\n",
    "<img src=\"Archer2.png\" class=centerimg50/>\n",
    "\n",
    "<img src=\"Archer3.png\" class=centerimg50/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043ee4fc",
   "metadata": {},
   "source": [
    "\n",
    "In the following, we will see some basic examples for HPC, such us\n",
    "\n",
    "-   Shared memory: with openmp\n",
    "-   Distributed memory: using mpi\n",
    "-   Multiple processes: using gnu parallel\n",
    "-   C++ threads\n",
    "-   TODO C++ parallel algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1afea7",
   "metadata": {},
   "source": [
    "\n",
    "## Openmp, shared memory\n",
    "\n",
    "The following code shows a very simple parallelization using openmp,\n",
    "which allows tu share memory and run on several threads.\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "  std::cout << \"BEFORE\\n\";\n",
    "#pragma omp parallel\n",
    "  {\n",
    "    std::cout << \"Hola mundo\\n\";\n",
    "  }\n",
    "  std::cout << \"AFTER\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "To activate multi-threading, compile it as\n",
    "\n",
    "``` bash\n",
    "g++ -fopenmp codes/openmp.cpp\n",
    "```\n",
    "\n",
    "To run it, you can control the number of threads using the environment\n",
    "variable `OMP_NUM_THREADS`:\n",
    "\n",
    "``` bash\n",
    "g++ -fopenmp codes/openmp.cpp\n",
    "echo \"Running with 2 threads\"\n",
    "OMP_NUM_THREADS=2 ./a.out\n",
    "echo \"Running with 4 threads\"\n",
    "OMP_NUM_THREADS=4 ./a.out\n",
    "```\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1.  Modify the previous exercise to identify the thread which is\n",
    "    printing. Find a function to get the \"thread id\".\n",
    "\n",
    "2.  Besides the thread id, print the number of threads and the hostname.\n",
    "    Print the number of threads outside the parallel region. Does that\n",
    "    make sense?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f83280",
   "metadata": {},
   "source": [
    "\n",
    "## MPI, distributed memory\n",
    "\n",
    "MPI, the Message Passing Interface, is a library API that allows process\n",
    "to interchange data in a distributed memory context. It is more comple\n",
    "that openmp, but also opens the door to a greater scale since we can use\n",
    "many computers, increasing both our computational power and memory\n",
    "capacity (if done correctly and efficiently).\n",
    "\n",
    "The following shows the basic structure of a MPI program. It creates\n",
    "several **processes** that can communicate with each other, and can be\n",
    "run in multiple machines (for an introduction, see:\n",
    "<https://mpitutorial.com/tutorials/mpi-introduction/>)\n",
    "\n",
    "``` cpp\n",
    "#include <mpi.h>\n",
    "#include <iostream>\n",
    "\n",
    "int main(int argc, char** argv)\n",
    "{\n",
    "    // Initialize the MPI environment\n",
    "    MPI_Init(&argc, &argv);\n",
    "\n",
    "    // Get the number of processes\n",
    "    int np;\n",
    "    MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
    "\n",
    "    // Get the rank of the process\n",
    "    int pid;\n",
    "    MPI_Comm_rank(MPI_COMM_WORLD, &pid);\n",
    "\n",
    "    // Get the name of the processor\n",
    "    char processor_name[MPI_MAX_PROCESSOR_NAME];\n",
    "    int name_len;\n",
    "    MPI_Get_processor_name(processor_name, &name_len);\n",
    "\n",
    "    // Print off a hello world message\n",
    "    printf(\"Hello world from processor %s, rank %d out of %d processes\\n\",\n",
    "           processor_name, pid, np);\n",
    "\n",
    "    // Finalize the MPI environment.\n",
    "    MPI_Finalize();\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "You can compile it as\n",
    "\n",
    "``` bash\n",
    "mpic++  mpi.cpp\n",
    "```\n",
    "\n",
    "(If you want to see all the flags, use `mpic++ --showme`)\n",
    "\n",
    "And now run it as\n",
    "\n",
    "``` bash\n",
    "mpirun -np 4 ./a.out\n",
    "```\n",
    "\n",
    "You can also specifiy different machines to run on, but you will need to\n",
    "have configured passwordless access to those machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2caf9a",
   "metadata": {},
   "source": [
    "\n",
    "## Simple parallelization: farm task and gnu parallel or xargs\n",
    "\n",
    "Sometimes you do not need to actually parallelize your code, but to run\n",
    "it with many parameters combination. Let's assume that we have a task\n",
    "that depend on one parameter and can be executed independent of other\n",
    "parameters. It can be a very complex program, but for now it will be\n",
    "just a very simple bash instructions that prints a value. Save the\n",
    "following code in a bash script (like `script.sh`) that will use the\n",
    "`stress` command to stress a single core\n",
    "\n",
    "``` bash\n",
    "# file: script.sh\n",
    "echo \"First arg: ${1}\"\n",
    "stress -t 10 -c 1 # stress one core\n",
    "echo \"Stress test done\"\n",
    "```\n",
    "\n",
    "When it is executed, it just prints the first argument\n",
    "\n",
    "``` bash\n",
    "bash codes/script.sh 23\n",
    "```\n",
    "\n",
    "What if we want to do execute this task for 4 different arguments? we\n",
    "will just do it sequentially:\n",
    "\n",
    "``` bash\n",
    "date +\"%H-%M-%S\"\n",
    "bash codes/script.sh 23\n",
    "bash codes/script.sh 42\n",
    "bash codes/script.sh 10\n",
    "bash codes/script.sh 57\n",
    "date +\"%H-%M-%S\"\n",
    "```\n",
    "\n",
    "40 seconds in total. Remember that this example is very simple, but\n",
    "assume that the script is a very large task. Then, the previous task\n",
    "will take four times the time of a simple task. What if we have a\n",
    "machine with four possible threads? it will be useful to run all the\n",
    "commands in parallel. To do so you might just put them in the background\n",
    "with the `&` character at the end. But what will happen if you need to\n",
    "run 7 different arguments and you have only 4 threads? then it would be\n",
    "not optimal to have all of them running at tha same time with less than\n",
    "100% of cpu usage. It would be better to run 4 of them and when one of\n",
    "the finishes then launch the next one and so on. To do this\n",
    "programatically, you can use `gnu parallel`,\n",
    "<https://www.gnu.org/software/parallel/> (check the tutorial in the\n",
    "documentation section, or the cheatsheet,\n",
    "<https://www.gnu.org/software/parallel/parallel_cheat.pdf>). You can\n",
    "install as `spack info parallel`, or load it with `spack load parallel`\n",
    "if it not installed already. For our case, it would be very useful\n",
    "\n",
    "``` bash\n",
    "date +\"%H-%M-%S\"\n",
    "parallel 'bash codes/script.sh {} ' ::: 23 42 10 57\n",
    "date +\"%H-%M-%S\"\n",
    "```\n",
    "\n",
    "|          |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "|----------|-------|-----------|-------------|-------|-----|------|-----|-----|-----|-----|-----|-----|\n",
    "| 08-12-25 |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 23        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[83775\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 42        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[83779\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 10        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[83781\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 57        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[83785\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| 08-12-36 |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "\n",
    "Around 10 seconds now! Gnu parallel will detect the number of cores and\n",
    "launch the process accodingly taking care of jobs distribution. Read the\n",
    "manual for the many options of this powerful tool that is used even on\n",
    "large clusters. For instance, try to run 7 processes:\n",
    "\n",
    "``` bash\n",
    "date +\"%H-%M-%S\"\n",
    "parallel 'bash codes/script.sh {} ' ::: 23 42 10 57 21 8 83\n",
    "date +\"%H-%M-%S\"\n",
    "```\n",
    "\n",
    "|          |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "|----------|-------|-----------|-------------|-------|-----|------|-----|-----|-----|-----|-----|-----|\n",
    "| 08-13-20 |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 23        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84082\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 42        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84086\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 10        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84088\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 57        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84091\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 21        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84161\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 8         |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84165\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| First    | arg:  | 83        |             |       |     |      |     |     |     |     |     |     |\n",
    "| stress:  | info: | \\[84168\\] | dispatching | hogs: | 1   | cpu, | 0   | io, | 0   | vm, | 0   | hdd |\n",
    "| Stress   | test  | done      |             |       |     |      |     |     |     |     |     |     |\n",
    "| 08-13-41 |       |           |             |       |     |      |     |     |     |     |     |     |\n",
    "\n",
    "You can play with the `-j n` flag to control how many jobs to run with\n",
    "parallel. By default it uses all possible threads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf562c51",
   "metadata": {},
   "source": [
    "\n",
    "### Using gnu parallel to run several matmul and compute metrics\n",
    "\n",
    "Now let's use the previous matmul with eigen and blas exercise to show\n",
    "how to use parallel to run several matrix sizes at the same time and\n",
    "also to compute some parallel metrics. The code is\n",
    "\n",
    "We have two goals:\n",
    "\n",
    "1.  To compute the wall time as a function of the matrix size (strong\n",
    "    scaling, changing problem size), using blas.\n",
    "2.  To compute the speedup and parallel efficiency for a fixed matrix\n",
    "    size.\n",
    "\n",
    "In the first case we will just use parallel to run as many simulations\n",
    "as possible. In the second case we will compute some metrics to check\n",
    "when is our code the most efficient.\n",
    "\n",
    "It is assumed that you have blas with spack, so you can load it as\n",
    "\n",
    "``` bash\n",
    "spack load openblas\n",
    "```\n",
    "\n",
    "or might have to install it using\n",
    "\n",
    "``` bash\n",
    "spack install openblas threads=openmp cxxflags=\"-O3\" cflags=\"-O3\" target=x86_64\n",
    "```\n",
    "\n",
    "(the last flag is just to have the same target independent of the actual\n",
    "machine, like the conditions we have in the computer room)\n",
    "\n",
    "1.  Strong scaling: Time as a function of matriz size, one thread,\n",
    "    eigen + blas\n",
    "\n",
    "    Naively, we could just run the program serially for each matriz\n",
    "    size, but if we are in computer with multiple cores/threads it would\n",
    "    be better if we ran as many parameters as possible (adapt the\n",
    "    following instructions to your case):\n",
    "\n",
    "    ``` bash\n",
    "    #source $HOME/repos/spack/share/spack/setup-env.sh\n",
    "    spack load openblas\n",
    "    #g++ -fopenmp -O3 -I $CMAKE_PREFIX_PATH/include -L $CMAKE_PREFIX_PATH/lib eigen-matmul.cpp -DEIGEN_USE_BLAS -lopenblas -o eigen_blas.x\n",
    "    g++ -fopenmp -O3  eigen-matmul.cpp -DEIGEN_USE_BLAS -lopenblas -o eigen_blas.x\n",
    "    parallel 'OMP_NUM_THREADS=1 ./eigen_blas.x {} 10 2>/dev//null' ::: 10 50 100 200 500 700 1000 2000 5000w\n",
    "    ```\n",
    "\n",
    "    Check that your programs are running in parallel, as expected.\n",
    "\n",
    "    1.  Exercise: Strong scaling for eigen eigenvectors\n",
    "\n",
    "        With the following code, compute the strong scaling of eigen\n",
    "        when computing eigenvectors with the more general method:\n",
    "\n",
    "        ``` cpp\n",
    "        #include <iostream>\n",
    "        #include <cstdlib>\n",
    "        #include <chrono>\n",
    "        #include <eigen3/Eigen/Dense>\n",
    "\n",
    "        void solve_eigensystem(int size, double &time);\n",
    "\n",
    "        int main(int arg, char **argv)\n",
    "        {\n",
    "          const int M = atoi(argv[1]); // Matrix size\n",
    "          const int R = atoi(argv[2]); // Repetitions\n",
    "          const int S = atoi(argv[3]); // seed\n",
    "          srand(S);\n",
    "\n",
    "          double totaltime = 0, auxtime = 0;\n",
    "          for(int irep = 0; irep < R; ++irep) {\n",
    "            solve_eigensystem(M, auxtime);\n",
    "            totaltime += auxtime;\n",
    "          }\n",
    "          std::cout << M << \"\\t\" << totaltime/R << \"\\n\";\n",
    "\n",
    "         }\n",
    "\n",
    "        void solve_eigensystem(int size, double &time)\n",
    "        {\n",
    "          double aux;\n",
    "          Eigen::MatrixXd A = Eigen::MatrixXd::Random(size, size);\n",
    "          auto start = std::chrono::steady_clock::now();\n",
    "          Eigen::SelfAdjointEigenSolver<Eigen::MatrixXd> eigensolver(A);\n",
    "          aux = eigensolver.eigenvalues()(0);\n",
    "          auto end = std::chrono::steady_clock::now();\n",
    "          std::clog << \"The first eigenvalue of A is:\\n\" << aux << std::endl;\n",
    "          std::chrono::duration<double> diff = end - start;\n",
    "          time = diff.count();\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        Compile como\n",
    "\n",
    "        ``` bash\n",
    "        g++ -O3 eigen-eigenvectors.cpp  -o eigen.x\n",
    "        ```\n",
    "\n",
    "        Ejecute como\n",
    "\n",
    "        ```bash\n",
    "        ./eigen.x M 5 3 2>/dev/null\n",
    "        ```\n",
    "\n",
    "        debe cambiar M, matriz size.\n",
    "\n",
    "        Datos de ejemplo (dependen de la maquina, pero las relaciones\n",
    "        entre ellos deben ser similares):\n",
    "\n",
    "        ``` bash\n",
    "        10      9.7568e-06\n",
    "        20      3.55734e-05\n",
    "        50      0.000312481\n",
    "        80      0.000890043\n",
    "        ...\n",
    "        2700    72.8078\n",
    "        3000    81.0619\n",
    "        ```\n",
    "\n",
    "        Plot the data ad analyze.\n",
    "\n",
    "2.  Weak scaling: number of threads and parallel metrics\n",
    "\n",
    "    Here we will compute some key parallel metrics that inform about the\n",
    "    efficiency of our code when running in parallel. Now you do not want\n",
    "    to use gnu parallel since you have a variable number of thredas per\n",
    "    process\n",
    "\n",
    "    ``` bash\n",
    "    source $HOME/repos/spack/share/spack/setup-env.sh\n",
    "    spack load openblas\n",
    "    #g++ -fopenmp -O3 -I $CMAKE_PREFIX_PATH/include -L $CMAKE_PREFIX_PATH/lib eigen-matmul.cpp -DEIGEN_USE_BLAS -lopenblas -o eigen_blas.x\n",
    "    g++ -fopenmp -O3 eigen-matmul.cpp -DEIGEN_USE_BLAS -lopenblas -o eigen_blas.x\n",
    "    parallel -j 1 'echo -n \"{}  \"; OMP_NUM_THREADS={} ./eigen_blas.x 4000 10 2>/dev//null' ::: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n",
    "    ```\n",
    "\n",
    "    The following is data obtained from a run in a 8core/16threads\n",
    "    computer, running only with eigen\n",
    "\n",
    "    ``` bash\n",
    "    1  4000 6.75223 0.00290168      6.75225 0.0029001\n",
    "    2  4000 3.52052 0.00405504      7.01616 0.00819132\n",
    "    3  4000 2.40281 0.0117795       7.15847 0.0355119\n",
    "    4  4000 1.85186 0.0049013       7.33257 0.0187681\n",
    "    5  4000 1.72682 0.176218        8.53451 0.880953\n",
    "    6  4000 1.65921 0.00946933      9.83127 0.0574367\n",
    "    7  4000 1.52068 0.00538196      10.4943 0.0370317\n",
    "    8  4000 1.39755 0.0326183       11.006  0.260568\n",
    "    9  4000 2.26355 0.00254841      19.9546 0.0452903\n",
    "    10  4000        2.04808 0.00732663      20.0991 0.0807175\n",
    "    11  4000        2.00821 0.00876695      21.7043 0.104527\n",
    "    12  4000        1.76768 0.0276189       20.834  0.324801\n",
    "    13  4000        1.77771 0.00686642      22.7412 0.0887671\n",
    "    14  4000        1.59293 0.0116353       21.9208 0.213236\n",
    "    15  4000        1.56692 0.00829185      23.1334 0.121202\n",
    "    16  4000        1.49262 0.0321579       23.3921 0.417985\n",
    "    ```\n",
    "\n",
    "    And these are with running with eigen+blas\n",
    "\n",
    "    ``` bash\n",
    "    1  4000 1.76345 0.00750705      1.76345 0.0075023\n",
    "    2  4000 0.930922        0.00450842      1.83688 0.00900039\n",
    "    3  4000 0.666528        0.0122499       1.94996 0.0365303\n",
    "    4  4000 0.523076        0.00175201      2.01795 0.00654245\n",
    "    5  4000 0.442096        0.00226719      2.1107  0.0108692\n",
    "    6  4000 0.394103        0.00867531      2.23982 0.0513271\n",
    "    7  4000 0.371224        0.000725666     2.44876 0.00691333\n",
    "    8  4000 0.35202 0.00564542      2.64095 0.0441576\n",
    "    9  4000 0.53266 0.00218486      4.59061 0.02803\n",
    "    10  4000        0.496156        0.00207424      4.73416 0.0281744\n",
    "    11  4000        0.461317        0.00102704      4.82462 0.0111843\n",
    "    12  4000        0.550406        0.0431109       6.32975 0.518631\n",
    "    13  4000        0.514583        0.0119228       6.38601 0.144949\n",
    "    14  4000        0.494073        0.0166192       6.58912 0.231588\n",
    "    15  4000        0.484151        0.0121776       6.90909 0.179303\n",
    "    16  4000        0.493364        0.0316198       7.45327 0.429279\n",
    "    ```\n",
    "\n",
    "    The two more important parallel metric efficiency.\n",
    "\n",
    "    The speed up is defined as\n",
    "\n",
    "    where $T_1$ is the reference time with one thread, and $T_n$ with n\n",
    "    threads. For a perfect scaling, $T_n = T_1/n$, so\n",
    "    $S_{\\rm theo}(n) = n$. This does not occur always and depend on the\n",
    "    actual machine, the communication patterns and so on. In the\n",
    "    following we will use $T_1 = 6.75223$ for eigen and $T_1 = 1.76345$\n",
    "    for eigen+blas.\n",
    "\n",
    "    The parallel efficiency measures roughly how efficiently we are\n",
    "    using all the threads. It is defined as\n",
    "\n",
    "    and, therefore, theoretically it is equal to 1.\n",
    "\n",
    "    To easily create the data we need, we could use a spreadsheet or\n",
    "    better, the command line\n",
    "\n",
    "    ``` bash\n",
    "    awk '{print $1, 6.75223/$3, 6.75223/$3/$1 }' codes/eigen.txt > codes/eigen_metrics.txt\n",
    "    awk '{print $1, 1.76345/$3, 1.76345/$3/$1 }' codes/eigen_blas.txt > codes/eigen_blas_metrics.txt\n",
    "    ```\n",
    "\n",
    "    Then we can plot and analize\n",
    "\n",
    "    ``` gnuplot\n",
    "    set term png enhaced giant#; set out 'tmpspeedup.png'\n",
    "    set key l t; set xlabel 'nthreads'; set ylabel 'Parallel speedup'; set title 'Computer with 8cores/16threads'\n",
    "    plot [:17][:] x lt -1 t 'theo', 'codes/eigen_metrics.txt' u 1:2 w lp t 'eigen', 'codes/eigen_blas_metrics.txt'  u 1:2 w lp t 'eigen+blas'\n",
    "    ```\n",
    "\n",
    "    [Speedup](speedup.png)\n",
    "\n",
    "    ``` gnuplot\n",
    "    #set term png; set out 'efficiency.png'\n",
    "    set key l b; set xlabel 'nthreads'; set ylabel 'Parallel efficiency'; set title 'Computer with 8cores/16threads'\n",
    "    plot [:17][-0.1:1.1] 1 lt -1, 'codes/eigen_metrics.txt' u 1:3 w lp t 'eigen', 'codes/eigen_blas_metrics.txt'  u 1:3 w lp t 'eigen+blas', 0.6 lt 4\n",
    "    ```\n",
    "\n",
    "    [Efficiency](efficiency.png)\n",
    "\n",
    "    Normally, if you want to buy some cloud services to run your code,\n",
    "    you should use threads that give equal or above 60-70% efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59178602",
   "metadata": {},
   "source": [
    "\n",
    "## Threads from c++11\n",
    "\n",
    "The `c++11` standard included, among many other usefull things, the use\n",
    "a **thread**. A thread is a lightweight process that can be launched in\n",
    "parallel with other threads from a parent process. In the following we\n",
    "will see some very simple examples since at the end we will focus mainly\n",
    "on OpenMP (where threads are the key and the memory is shared) and MPI\n",
    "(where processes are the basic unit and memory is distributed).\n",
    "\n",
    "The following example are based on\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/thread>\n",
    "-   <https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab6/>\n",
    "\n",
    "The following example shows how to create a thread from a given process,\n",
    "and its output:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "\n",
    "void func(int x);\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    std::thread th1(&func, 100);\n",
    "    std::thread th2(&func, 200);\n",
    "    th1.join();\n",
    "    std::cout << \"Outside thread\" << std::endl;\n",
    "    th2.join();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void func(int x) {\n",
    "    std::cout << \"Inside thread \" << x << std::endl;\n",
    "    std::thread::id this_id = std::this_thread::get_id();\n",
    "    std::cout << \"This is thread_id: \" << this_id << std::endl;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile it as\n",
    "\n",
    "``` bash\n",
    "g++ -std=c++11 thread-v1.cpp\n",
    "```\n",
    "\n",
    "The folowwing is an example of the output:\n",
    "\n",
    "|                |        |                      |        |        |                      |                |\n",
    "|----------------|--------|----------------------|--------|--------|----------------------|----------------|\n",
    "| Inside         | thread | Inside               | thread | 100200 |                      |                |\n",
    "|                |        |                      |        |        |                      |                |\n",
    "| This           | is     | thread<sub>id</sub>: | This   | is     | thread<sub>id</sub>: | 0x700003c34000 |\n",
    "| 0x700003cb7000 |        |                      |        |        |                      |                |\n",
    "| Outside        | thread |                      |        |        |                      |                |\n",
    "\n",
    "Run it several times, you will obtain different outputs, many times they\n",
    "will be mangled. Why? because the threads are running in parallel and\n",
    "their output is not independent of each other, not synced.\n",
    "\n",
    "To check that we are really running two threads, let's increase the\n",
    "computational effort inside function `func` and then, while the program\n",
    "is running, use top or htop to check what is running on your computer.\n",
    "Notice that the cpu use percentage is around `200%`:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <chrono>\n",
    "#include <cmath>\n",
    "\n",
    "void func(double x, int nsecs);\n",
    "\n",
    "int main(int argc, char **argv) {\n",
    "    const int secs = std::atoi(argv[1]);\n",
    "    std::thread th1(&func, 100, secs);\n",
    "    std::thread th2(&func, 200, secs);\n",
    "    std::thread th3(&func, 300, secs);\n",
    "    th1.join();\n",
    "    std::cout << \"Outside thread\" << std::endl;\n",
    "    th2.join();\n",
    "    th3.join();\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void func(double x, int nsecs) {\n",
    "    std::cout << \"Inside thread \" << x << std::endl;\n",
    "    std::this_thread::sleep_for (std::chrono::seconds(nsecs)); // make this sleep, does not consume a lot of resources\n",
    "    for (int ii = 0; ii < 100000000; ++ii) {\n",
    "        x += std::fabs(x*std::sin(x) + std::sqrt(x)/3.4455)/(ii+1);\n",
    "    }\n",
    "    std::cout << \"Getting out of thread \" << x << std::endl;\n",
    "}\n",
    "```\n",
    "\n",
    "The folowwing is an example of the output:\n",
    "\n",
    "|         |        |        |        |               |\n",
    "|---------|--------|--------|--------|---------------|\n",
    "| Inside  | thread | Inside | thread | 100           |\n",
    "| 200     |        |        |        |               |\n",
    "| Getting | out    | of     | thread | 9387820000.0  |\n",
    "| Outside | thread |        |        |               |\n",
    "| Getting | out    | of     | thread | 18849600000.0 |\n",
    "\n",
    "To synchronize threads, you can use a mutex. This is useful in case you\n",
    "need to sort out the printing, or, more importantly, to syncrhonize a\n",
    "writing operation on some common variable. The following is a simple\n",
    "example:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <thread>\n",
    "#include <chrono>\n",
    "#include <mutex>\n",
    "\n",
    "std::mutex g_display_mutex;\n",
    "\n",
    "void foo()\n",
    "{\n",
    "    std::thread::id this_id = std::this_thread::get_id();\n",
    "\n",
    "    g_display_mutex.lock();\n",
    "    std::cout << \"thread \" << this_id << \" sleeping...\\n\";\n",
    "    g_display_mutex.unlock();\n",
    "\n",
    "    std::this_thread::sleep_for(std::chrono::seconds(1));\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    std::thread t1(foo);\n",
    "    std::thread t2(foo);\n",
    "\n",
    "    t1.join();\n",
    "    t2.join();\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "|        |                |           |\n",
    "|--------|----------------|-----------|\n",
    "| thread | 0x70000709a000 | sleeping… |\n",
    "| thread | 0x70000711d000 | sleeping… |\n",
    "\n",
    "Repeat several times. Although the thread id will change, the output\n",
    "will not be mangled.\n",
    "\n",
    "There is much more about threads, but since our focus will turn to\n",
    "OpenMP, we will stop here. For more info check\n",
    "<https://en.cppreference.com/w/cpp/thread/thread>\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Fix the following code, which has a race condition, using a mutex (ref:\n",
    "<https://www.classes.cs.uchicago.edu/archive/2013/spring/12300-1/labs/lab6/>)\n",
    ":\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <thread>\n",
    "\n",
    "void square(const int x, int & result);\n",
    "\n",
    "int main() {\n",
    "    int accum = 0;\n",
    "    std::vector<std::thread> ths;\n",
    "    for (int i = 1; i <= 20; i++) {\n",
    "        ths.push_back(std::thread(&square, i, std::ref(accum)));\n",
    "    }\n",
    "\n",
    "    for (auto & th : ths) {\n",
    "        th.join();\n",
    "    }\n",
    "    std::cout << \"accum = \" << accum << std::endl;\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "void square(int x, int &result) {\n",
    "    result += x * x;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "``` bash\n",
    "accum = 2870\n",
    "```\n",
    "\n",
    "The correct answer is `2870`, but if you repeat the execution many times\n",
    "you will find different results. For instance, repeating the execution\n",
    "1000 times and checking for the unique answers one gets\n",
    "\n",
    "``` bash\n",
    "for i in {1..1000}; do ./a.out; done | sort | uniq -c\n",
    "```\n",
    "\n",
    "``` bash\n",
    "  2 accum = 2509\n",
    "  1 accum = 2674\n",
    "  2 accum = 2749\n",
    "  1 accum = 2806\n",
    "  1 accum = 2834\n",
    "  4 accum = 2845\n",
    "  1 accum = 2854\n",
    "  1 accum = 2861\n",
    "  2 accum = 2866\n",
    "  6 accum = 2869\n",
    "979 accum = 2870\n",
    "```\n",
    "\n",
    "which shows that it not always yield 2870 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fa7199",
   "metadata": {},
   "source": [
    "\n",
    "## Parallel algorithms in c++\n",
    "\n",
    "Since `c++17`, it is possible to execute some stl algorithms in parallel\n",
    "(shared memory), without explictly using threads. See:\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t>\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm>\n",
    "-   <https://en.cppreference.com/w/cpp/experimental/parallelism>\n",
    "\n",
    "This a simple example for performing a sum\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "\n",
    "int main() {\n",
    "    const long ARRAY_SIZE = 100000000;\n",
    "    std::vector<double> myArray(ARRAY_SIZE);\n",
    "    std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "    // sequential execution\n",
    "    auto sum_seq = std::accumulate(myArray.begin(), myArray.end(), 0.0);\n",
    "    std::cout << \"Sequential sum: \" << sum_seq << std::endl;\n",
    "\n",
    "    // parallel execution\n",
    "    auto sum_par = std::reduce(std::execution::par, myArray.begin(), myArray.end());\n",
    "    std::cout << \"Parallel sum: \" << sum_par << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "To compile, use\n",
    "\n",
    "``` bash\n",
    "g++ -std=c++17 par.cpp -ltbb\n",
    "```\n",
    "\n",
    "This is linking with an intel threads implementation.\n",
    "\n",
    "You can of course measure how much time is spent on each part. To do so,\n",
    "we will use chrono:\n",
    "\n",
    "-   Implementation 1:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "\n",
    "int main() {\n",
    "    const long ARRAY_SIZE = 100000000;\n",
    "    std::vector<double> myArray(ARRAY_SIZE);\n",
    "    std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "    // sequential execution\n",
    "    auto start_time = std::chrono::high_resolution_clock::now();\n",
    "    auto sum_seq = std::accumulate(myArray.begin(), myArray.end(), 0.0);\n",
    "    auto end_time = std::chrono::high_resolution_clock::now();\n",
    "    auto seq_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n",
    "    std::cout << \"Sequential sum: \" << sum_seq << \"( took : \" << seq_duration.count()/1000.0 << \" s)\" << std::endl;\n",
    "\n",
    "    // parallel execution\n",
    "    start_time = std::chrono::high_resolution_clock::now();\n",
    "    auto sum_par = std::reduce(std::execution::par, myArray.begin(), myArray.end());\n",
    "    end_time = std::chrono::high_resolution_clock::now();\n",
    "    seq_duration = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time);\n",
    "    std::cout << \"Parallel sum: \" << sum_par << \"( took : \" << seq_duration.count()/1000.0 << \" s)\" << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "-   Implementation 2:\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <algorithm>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "#include <execution>\n",
    "#include <chrono>\n",
    "\n",
    "template<typename Func>\n",
    "void time_function(Func func);\n",
    "\n",
    "int main() {\n",
    "  const long ARRAY_SIZE = 100000000;\n",
    "  std::vector<double> myArray(ARRAY_SIZE);\n",
    "  std::iota(myArray.begin(), myArray.end(), 0); // fill array with 0, 1, 2, ..., ARRAY_SIZE-1\n",
    "\n",
    "  // sequential execution\n",
    "  auto serial = [&myArray](){return std::accumulate(myArray.begin(), myArray.end(), 0.0);};\n",
    "  time_function(serial);\n",
    "\n",
    "  // parallel execution\n",
    "  auto parallel = [&myArray](){return std::reduce(std::execution::par, myArray.begin(), myArray.end());};\n",
    "  time_function(parallel);\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "template<typename Func>\n",
    "void time_function(Func func) {\n",
    "  auto start = std::chrono::high_resolution_clock::now();\n",
    "  func();\n",
    "  auto end = std::chrono::high_resolution_clock::now();\n",
    "  auto duration_ms = std::chrono::duration_cast<std::chrono::milliseconds>(end - start).count();\n",
    "  std::cout << \"Elapsed time: \" << duration_ms/1000.0 << \"  s\" << std::endl;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "The standard does not specify a way to control the nuber tof threads. If\n",
    "you want to do so, and you are using Intel Threads Block implementation,\n",
    "you can add gthe following header\n",
    "\n",
    "``` cpp\n",
    "#include <tbb/task_scheduler_init.h>\n",
    "```\n",
    "\n",
    "and then , at some point, specify the thread total (in this case, 4)\n",
    "\n",
    "``` cpp\n",
    "tbb::task_scheduler_init init(4);\n",
    "```\n",
    "\n",
    "To learn more about the parallel algs, check\n",
    "\n",
    "-   <https://en.cppreference.com/w/cpp/algorithm>\n",
    "\n",
    "## <span class=\"todo TODO\">TODO</span> Gpu Programming intro\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe31eff",
   "metadata": {},
   "source": [
    "\n",
    "### Environment setup for cuda\n",
    "\n",
    "Here I show two ways to setup the dev environment. One is based on a\n",
    "local computer with a graphics card, and the other using google collab.\n",
    "\n",
    "1.  Local environment\n",
    "\n",
    "    Here we will setup a computer which has an Nvidia Quadro P1000 card.\n",
    "    You need to install both the driver and the cuda toolkit (the later\n",
    "    better to be installed as a part of the nvidia sdk)\n",
    "\n",
    "    -   Driver download for quadro P1000:\n",
    "        <https://www.nvidia.com/Download/driverResults.aspx/204639/en-us/>\n",
    "    -   Nvidia sdk: <https://developer.nvidia.com/hpc-sdk-downloads>\n",
    "        -   Nvidia singularity: This is the recommended way. The image\n",
    "            is built at\n",
    "            /packages/nvhpc<sub>23</sub>.3<sub>devel</sub>.sif. More\n",
    "            instructions at\n",
    "            <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc>\n",
    "\n",
    "            1.  Accesing a shell inside the container but with\n",
    "                visibility to all user account files:\n",
    "\n",
    "                ``` bash\n",
    "                singularity shell --nv /packages/nvhpc_23.3_devel.sif\n",
    "                ```\n",
    "\n",
    "            2.  Compiling\n",
    "\n",
    "                ``` bash\n",
    "                singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "                ```\n",
    "\n",
    "            3.  Executing with nvprof\n",
    "\n",
    "                ``` bash\n",
    "                singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "                ```\n",
    "\n",
    "        -   Local module: Load the nvidia sdk (sala2):\n",
    "\n",
    "            ``` bash\n",
    "            module load /packages/nvidia/hpc_sdk/modulefiles/nvhpc/23.3\n",
    "            ```\n",
    "\n",
    "            Compile as\n",
    "\n",
    "            ``` bash\n",
    "            nvc++  -std=c++17 -o offload.x offload.cpp\n",
    "            ```\n",
    "\n",
    "        -   The docker container is installed. Unfortunately it does not\n",
    "            run since the device compute capability is not enough\n",
    "\n",
    "            ``` bash\n",
    "            docker run --gpus all -it --rm nvcr.io/nvidia/nvhpc:23.3-devel-cuda_multi-ubuntu20.04\n",
    "            docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n",
    "            ```\n",
    "\n",
    "            More info about container:\n",
    "            <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc>\n",
    "\n",
    "2.  Google collab\n",
    "\n",
    "    Open a collab notebook, go to runtime, change runtime type, hardware\n",
    "    accelerator -\\> GPU, GPU type -\\> T4, Save. The you will have a\n",
    "    runtime with a T4 card, for free. If you want an even better card,\n",
    "    you can pay for collab pro.\n",
    "\n",
    "    Inside the notebook, you can run commands with the prefix `!` to run\n",
    "    then as in a console. For instance, to get the device properties,\n",
    "    you can run\n",
    "\n",
    "    ``` bash\n",
    "    !nvidia-smi\n",
    "    ```\n",
    "\n",
    "    to get something like\n",
    "\n",
    "    ``` bash\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
    "    |-------------------------------+----------------------+----------------------+\n",
    "    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "    |                               |                      |               MIG M. |\n",
    "    |===============================+======================+======================|\n",
    "    |   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "    | N/A   44C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | Processes:                                                                  |\n",
    "    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "    |        ID   ID                                                   Usage      |\n",
    "    |=============================================================================|\n",
    "    |  No running processes found                                                 |\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    ```\n",
    "\n",
    "    To create local files, like `filename.cu`, use the magic\n",
    "    `%%writefile\n",
    "    filename.cu` at the beginning of the cell and then put the file\n",
    "    contents in the same cell.\n",
    "\n",
    "    Finally, to compile and run just execute the following\n",
    "\n",
    "    ``` bash\n",
    "    !nvcc filename.cu -o name.x\n",
    "    !nvprof ./name.x\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a2b51",
   "metadata": {},
   "source": [
    "\n",
    "### <span class=\"todo TODO\">TODO</span> Cuda intro\n",
    "\n",
    "REF <https://en.wikipedia.org/wiki/CUDA?useskin=vector> Tutorial1:\n",
    "<https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/>\n",
    "Tutorial2\n",
    "<https://developer.nvidia.com/blog/even-easier-introduction-cuda/>\n",
    "\n",
    "1.  Tutorial 1\n",
    "\n",
    "    <https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/>\n",
    "    Example in c\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    gcc example_01.c\n",
    "    ```\n",
    "\n",
    "    Now the same but in cuda:\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    nvcc example_01.cu\n",
    "    ```\n",
    "\n",
    "    Execution will show errors, due to the fact that the code is NOT\n",
    "    running on the device.\n",
    "\n",
    "    We need to allocate memory on it(`cudaMalloc` and `cudaFree`), and\n",
    "    trasfer data to and from it (`cudaMemCopy`).\n",
    "\n",
    "2.  Tutorial 2\n",
    "\n",
    "    <https://developer.nvidia.com/blog/even-easier-introduction-cuda/>\n",
    "\n",
    "    ``` cpp\n",
    "    #include <iostream>\n",
    "    #include <math.h>\n",
    "\n",
    "    // function to add the elements of two arrays\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      for (int i = 0; i < n; i++)\n",
    "          y[i] = x[i] + y[i];\n",
    "    }\n",
    "\n",
    "    int main(void)\n",
    "    {\n",
    "      int N = 1<<20; // 1M elements\n",
    "\n",
    "      float *x = new float[N];\n",
    "      float *y = new float[N];\n",
    "\n",
    "      // initialize x and y arrays on the host\n",
    "      for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "      }\n",
    "\n",
    "      // Run kernel on 1M elements on the CPU\n",
    "      add(N, x, y);\n",
    "\n",
    "      // Check for errors (all values should be 3.0f)\n",
    "      float maxError = 0.0f;\n",
    "      for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "      std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "      // Free memory\n",
    "      delete [] x;\n",
    "      delete [] y;\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    g++ -g -std=c++17 cuda_01.cpp\n",
    "    ```\n",
    "\n",
    "    Cuda example\n",
    "\n",
    "    ``` cuda\n",
    "    #include <iostream>\n",
    "    #include <math.h>\n",
    "    // Kernel function to add the elements of two arrays\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "    }\n",
    "\n",
    "    int main(void)\n",
    "    {\n",
    "      int N = 1<<20;\n",
    "      float *x, *y;\n",
    "\n",
    "      // Allocate Unified Memory – accessible from CPU or GPU\n",
    "      cudaMallocManaged(&x, N*sizeof(float));\n",
    "      cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "      // initialize x and y arrays on the host\n",
    "      for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "      }\n",
    "\n",
    "      // Run kernel on 1M elements on the GPU\n",
    "      add<<<1, 1>>>(N, x, y);\n",
    "\n",
    "      // Wait for GPU to finish before accessing on host\n",
    "      cudaDeviceSynchronize();\n",
    "\n",
    "      // Check for errors (all values should be 3.0f)\n",
    "      float maxError = 0.0f;\n",
    "      for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "      std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "      // Free memory\n",
    "      cudaFree(x);\n",
    "      cudaFree(y);\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    To compile, use `nvc++`.\n",
    "\n",
    "    If you have a singularity container with the nvidia sdk, you can\n",
    "    just run the following\n",
    "\n",
    "    ``` bash\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif ./a.out\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "    ```\n",
    "\n",
    "    and get something like\n",
    "\n",
    "    ``` bash\n",
    "    ==16094== NVPROF is profiling process 16094, command: ./a.out\n",
    "    Max error: 0\n",
    "    ==16094== Profiling application: ./a.out\n",
    "    ==16094== Profiling result:\n",
    "                Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "     GPU activities:  100.00%  2.54774s         1  2.54774s  2.54774s  2.54774s  add(int, float*, float*)\n",
    "          API calls:   93.27%  2.54776s         1  2.54776s  2.54776s  2.54776s  cudaDeviceSynchronize\n",
    "                        6.71%  183.20ms         2  91.602ms  20.540us  183.18ms  cudaMallocManaged\n",
    "                        0.02%  468.25us         2  234.13us  216.27us  251.98us  cudaFree\n",
    "                        0.01%  213.75us       101  2.1160us     141ns  150.11us  cuDeviceGetAttribute\n",
    "                        0.00%  32.127us         1  32.127us  32.127us  32.127us  cudaLaunchKernel\n",
    "                        0.00%  22.239us         1  22.239us  22.239us  22.239us  cuDeviceGetName\n",
    "                        0.00%  6.1330us         1  6.1330us  6.1330us  6.1330us  cuDeviceGetPCIBusId\n",
    "                        0.00%  1.5730us         3     524ns     197ns  1.1650us  cuDeviceGetCount\n",
    "                        0.00%     808ns         2     404ns     141ns     667ns  cuDeviceGet\n",
    "                        0.00%     530ns         1     530ns     530ns     530ns  cuDeviceTotalMem\n",
    "                        0.00%     243ns         1     243ns     243ns     243ns  cuDeviceGetUuid\n",
    "\n",
    "    ==16094== Unified Memory profiling result:\n",
    "    Device \"Quadro P1000 (0)\"\n",
    "       Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "          48  170.67KB  4.0000KB  0.9961MB  8.000000MB  735.2380us  Host To Device\n",
    "          24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.3770us  Device To Host\n",
    "          24         -         -         -           -  2.855987ms  Gpu page fault groups\n",
    "    Total CPU Page faults: 36\n",
    "    ```\n",
    "\n",
    "    You can also run it on google collab, where you will have an nvidia\n",
    "    T4 card available for free (after changing the runtime), with the\n",
    "    following typical output\n",
    "\n",
    "    ``` bash\n",
    "    ==18853== NVPROF is profiling process 18853, command: ./cuda_02.x\n",
    "    Max error: 0\n",
    "    ==18853== Profiling application: ./cuda_02.x\n",
    "    ==18853== Profiling result:\n",
    "                Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "     GPU activities:  100.00%  108.83ms         1  108.83ms  108.83ms  108.83ms  add(int, float*, float*)\n",
    "          API calls:   72.48%  290.34ms         2  145.17ms  36.191us  290.31ms  cudaMallocManaged\n",
    "                       27.17%  108.84ms         1  108.84ms  108.84ms  108.84ms  cudaDeviceSynchronize\n",
    "                        0.28%  1.1298ms         2  564.90us  537.96us  591.84us  cudaFree\n",
    "                        0.05%  182.13us       101  1.8030us     264ns  75.268us  cuDeviceGetAttribute\n",
    "                        0.01%  48.553us         1  48.553us  48.553us  48.553us  cudaLaunchKernel\n",
    "                        0.01%  28.488us         1  28.488us  28.488us  28.488us  cuDeviceGetName\n",
    "                        0.00%  8.6520us         1  8.6520us  8.6520us  8.6520us  cuDeviceGetPCIBusId\n",
    "                        0.00%  2.3140us         3     771ns     328ns  1.6230us  cuDeviceGetCount\n",
    "                        0.00%     919ns         2     459ns     315ns     604ns  cuDeviceGet\n",
    "                        0.00%     580ns         1     580ns     580ns     580ns  cuDeviceTotalMem\n",
    "                        0.00%     532ns         1     532ns     532ns     532ns  cuModuleGetLoadingMode\n",
    "                        0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
    "\n",
    "    ==18853== Unified Memory profiling result:\n",
    "    Device \"Tesla T4 (0)\"\n",
    "       Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "          48  170.67KB  4.0000KB  0.9961MB  8.000000MB  809.9640us  Host To Device\n",
    "          24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.6320us  Device To Host\n",
    "          12         -         -         -           -  2.564287ms  Gpu page fault groups\n",
    "    Total CPU Page faults: 36\n",
    "    ```\n",
    "\n",
    "    If you increase just the number of threads to 256 (check the change\n",
    "    in `<<<...>>>`), and split correctly the work using the cuda vars\n",
    "    `threadIdx.x` (thread id in the block) and `blockDim.x` (number of\n",
    "    threads in the block), as shown,\n",
    "\n",
    "    ``` cuda\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      int index = threadIdx.x;\n",
    "      int stride = blockDim.x;\n",
    "      for (int i = index; i < n; i += stride)\n",
    "          y[i] = x[i] + y[i];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ``` cuda\n",
    "    // Run kernel on 1M elements on the GPU\n",
    "      add<<<1, 256>>>(N, x, y);\n",
    "    ```\n",
    "\n",
    "    then you get the following output\n",
    "\n",
    "    -   Quadro P1000 : From 2.5 secs to 0.022 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==21739== NVPROF is profiling process 21739, command: ./a.out\n",
    "        Max error: 0\n",
    "        ==21739== Profiling application: ./a.out\n",
    "        ==21739== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  21.978ms         1  21.978ms  21.978ms  21.978ms  add(int, float*, float*)\n",
    "              API calls:   87.86%  164.24ms         2  82.118ms  12.398us  164.22ms  cudaMallocManaged\n",
    "                           11.76%  21.980ms         1  21.980ms  21.980ms  21.980ms  cudaDeviceSynchronize\n",
    "                            0.24%  457.32us         2  228.66us  177.89us  279.43us  cudaFree\n",
    "                            0.11%  206.80us       101  2.0470us     128ns  144.81us  cuDeviceGetAttribute\n",
    "                            0.02%  29.041us         1  29.041us  29.041us  29.041us  cudaLaunchKernel\n",
    "                            0.01%  20.149us         1  20.149us  20.149us  20.149us  cuDeviceGetName\n",
    "                            0.00%  5.5860us         1  5.5860us  5.5860us  5.5860us  cuDeviceGetPCIBusId\n",
    "                            0.00%  2.1000us         3     700ns     277ns     958ns  cuDeviceGetCount\n",
    "                            0.00%     952ns         2     476ns     330ns     622ns  cuDeviceGet\n",
    "                            0.00%     391ns         1     391ns     391ns     391ns  cuDeviceTotalMem\n",
    "                            0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n",
    "\n",
    "        ==21739== Unified Memory profiling result:\n",
    "        Device \"Quadro P1000 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              48  170.67KB  4.0000KB  0.9961MB  8.000000MB  734.5940us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  338.5950us  Device To Host\n",
    "              24         -         -         -           -  1.764587ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    -   Tesla T4: From 0.108 secs to 0.004 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==21448== NVPROF is profiling process 21448, command: ./cuda_03.x\n",
    "        Max error: 0\n",
    "        ==21448== Profiling application: ./cuda_03.x\n",
    "        ==21448== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  3.7978ms         1  3.7978ms  3.7978ms  3.7978ms  add(int, float*, float*)\n",
    "              API calls:   98.24%  291.22ms         2  145.61ms  73.005us  291.15ms  cudaMallocManaged\n",
    "                            1.28%  3.8044ms         1  3.8044ms  3.8044ms  3.8044ms  cudaDeviceSynchronize\n",
    "                            0.36%  1.0699ms         2  534.95us  512.29us  557.62us  cudaFree\n",
    "                            0.08%  222.64us       101  2.2040us     174ns  102.62us  cuDeviceGetAttribute\n",
    "                            0.02%  62.588us         1  62.588us  62.588us  62.588us  cudaLaunchKernel\n",
    "                            0.02%  44.725us         1  44.725us  44.725us  44.725us  cuDeviceGetName\n",
    "                            0.00%  8.1290us         1  8.1290us  8.1290us  8.1290us  cuDeviceGetPCIBusId\n",
    "                            0.00%  3.2970us         3  1.0990us     266ns  2.6840us  cuDeviceGetCount\n",
    "                            0.00%  1.7320us         2     866ns     352ns  1.3800us  cuDeviceGet\n",
    "                            0.00%     632ns         1     632ns     632ns     632ns  cuDeviceTotalMem\n",
    "                            0.00%     549ns         1     549ns     549ns     549ns  cuModuleGetLoadingMode\n",
    "                            0.00%     377ns         1     377ns     377ns     377ns  cuDeviceGetUuid\n",
    "\n",
    "        ==21448== Unified Memory profiling result:\n",
    "        Device \"Tesla T4 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              48  170.67KB  4.0000KB  0.9961MB  8.000000MB  825.8720us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.3130us  Device To Host\n",
    "              13         -         -         -           -  2.951606ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    Cuda devices group parallel processors into Streaming\n",
    "    Multiprocessors (SM), and each of them can run several threads in\n",
    "    parallel. In our case, by using the command `deviceQuery` (for the\n",
    "    QuadroP1000 system it is at\n",
    "    `/opt/cuda/extras/demo_suite/deviceQuery`), we get\n",
    "\n",
    "    -   Quadro P1000: 5 SM, 128 threads/SM\n",
    "    -   Tesla T4: 32 SM, 128 threads/SM\n",
    "\n",
    "    So the ideal number of threads changes per card, and we will compute\n",
    "    as\n",
    "\n",
    "    ``` cpp\n",
    "    int blockSize = 128;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize; // what if N is not divisible by blocksize?\n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    ```\n",
    "\n",
    "    Notice that you can also compute this constant by using the follow\n",
    "    code (generated by bard.google.com)\n",
    "\n",
    "    ``` cpp\n",
    "    // Get the number of threads per multiprocessor.\n",
    "    int threadsPerMultiprocessor;\n",
    "    cudaError_t err = cudaDeviceGetAttribute(&threadsPerMultiprocessor, cudaDevAttrMaxThreadsPerMultiprocessor, device);\n",
    "    if (err != cudaSuccess) {\n",
    "      // Handle error.\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    The kernel will now become\n",
    "\n",
    "    ``` cpp\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "      int stride = blockDim.x * gridDim.x;\n",
    "      for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    based on the job distribution done by the tutorial\n",
    "    <https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png>\n",
    "\n",
    "    Now we get\n",
    "\n",
    "    -   Nvidia Quadro P1000: From 2.500 to 0.022 to 0.006 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==10662== NVPROF is profiling process 10662, command: ./a.out\n",
    "        Max error: 0\n",
    "        ==10662== Profiling application: ./a.out\n",
    "        ==10662== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  6.0868ms         1  6.0868ms  6.0868ms  6.0868ms  add(int, float*, float*)\n",
    "              API calls:   96.03%  165.28ms         2  82.641ms  13.911us  165.27ms  cudaMallocManaged\n",
    "                            3.54%  6.0887ms         1  6.0887ms  6.0887ms  6.0887ms  cudaDeviceSynchronize\n",
    "                            0.27%  460.56us         2  230.28us  184.71us  275.85us  cudaFree\n",
    "                            0.13%  215.37us       101  2.1320us     133ns  151.55us  cuDeviceGetAttribute\n",
    "                            0.02%  30.822us         1  30.822us  30.822us  30.822us  cudaLaunchKernel\n",
    "                            0.01%  22.122us         1  22.122us  22.122us  22.122us  cuDeviceGetName\n",
    "                            0.00%  5.7430us         1  5.7430us  5.7430us  5.7430us  cuDeviceGetPCIBusId\n",
    "                            0.00%  1.3810us         3     460ns     203ns     945ns  cuDeviceGetCount\n",
    "                            0.00%     921ns         2     460ns     163ns     758ns  cuDeviceGet\n",
    "                            0.00%     438ns         1     438ns     438ns     438ns  cuDeviceTotalMem\n",
    "                            0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n",
    "\n",
    "        ==10662== Unified Memory profiling result:\n",
    "        Device \"Quadro P1000 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              59  138.85KB  4.0000KB  0.9961MB  8.000000MB  740.3880us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.8280us  Device To Host\n",
    "              32         -         -         -           -  2.253582ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    -   Testla T4: From 0.108 to 0.004 to 0.003 secs\n",
    "\n",
    "        ``` bash\n",
    "        ==8972== NVPROF is profiling process 8972, command: ./cuda_04.x\n",
    "        Max error: 0\n",
    "        ==8972== Profiling application: ./cuda_04.x\n",
    "        ==8972== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  2.9741ms         1  2.9741ms  2.9741ms  2.9741ms  add(int, float*, float*)\n",
    "              API calls:   98.47%  250.63ms         2  125.31ms  38.785us  250.59ms  cudaMallocManaged\n",
    "                            1.18%  2.9959ms         1  2.9959ms  2.9959ms  2.9959ms  cudaDeviceSynchronize\n",
    "                            0.24%  613.16us         2  306.58us  302.27us  310.89us  cudaFree\n",
    "                            0.07%  188.26us       101  1.8630us     169ns  86.068us  cuDeviceGetAttribute\n",
    "                            0.02%  38.874us         1  38.874us  38.874us  38.874us  cuDeviceGetName\n",
    "                            0.01%  37.051us         1  37.051us  37.051us  37.051us  cudaLaunchKernel\n",
    "                            0.00%  5.7050us         1  5.7050us  5.7050us  5.7050us  cuDeviceGetPCIBusId\n",
    "                            0.00%  2.2980us         3     766ns     224ns  1.8050us  cuDeviceGetCount\n",
    "                            0.00%     979ns         2     489ns     195ns     784ns  cuDeviceGet\n",
    "                            0.00%     587ns         1     587ns     587ns     587ns  cuDeviceTotalMem\n",
    "                            0.00%     367ns         1     367ns     367ns     367ns  cuModuleGetLoadingMode\n",
    "                            0.00%     324ns         1     324ns     324ns     324ns  cuDeviceGetUuid\n",
    "\n",
    "        ==8972== Unified Memory profiling result:\n",
    "        Device \"Tesla T4 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "             106  77.282KB  4.0000KB  980.00KB  8.000000MB  969.6510us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  363.6760us  Device To Host\n",
    "              11         -         -         -           -  2.908132ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "### <span class=\"todo TODO\">TODO</span> Openmp offload to gpu\n",
    "\n",
    "REF:\n",
    "\n",
    "-   <https://www.youtube.com/watch?v=uVcvecgdW7g>\n",
    "\n",
    "Code\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <cstdio>\n",
    "#include <omp.h>\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int a[100], b[100], c[100];\n",
    "    int i;\n",
    "\n",
    "    // Initialize arrays a and b\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        a[i] = i;\n",
    "        b[i] = 2 * i;\n",
    "    }\n",
    "\n",
    "    int num_devices = omp_get_num_devices();\n",
    "    printf(\"Number of available devices %d\\n\", num_devices);\n",
    "\n",
    "    // Offload computation to GPU\n",
    "    #pragma omp target teams distribute parallel for map(to:a[0:100], b[0:100]) map(from:c[0:100])\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "\n",
    "    // Print results\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        std::cout << c[i] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Does not work in sala2 due to the error\n",
    "\n",
    "``` bash\n",
    "OpenMP GPU Offload is available only on systems with NVIDIA GPUs with compute capability '>= cc70\n",
    "```\n",
    "\n",
    "It seems that sala2 compute capability is 6.1. It can be get with\n",
    "\n",
    "``` bash\n",
    "nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "```\n",
    "\n",
    "Using google collab I can compile it\n",
    "\n",
    "``` bash\n",
    "!nvcc -arch sm_75 -O3 -o openmp_offload openmp_offload.cpp -lgomp\n",
    "```\n",
    "\n",
    "and get\n",
    "\n",
    "``` bash\n",
    "Number of available devices 1\n",
    "0 3 6 9 12 15 18 21 24 27 30 33 36 ...\n",
    "```\n",
    "\n",
    "Check:\n",
    "\n",
    "-   <https://enccs.github.io/openmp-gpu/target/>\n",
    "\n",
    "``` c\n",
    "/* Copyright (c) 2019 CSC Training */\n",
    "/* Copyright (c) 2021 ENCCS */\n",
    "#include <stdio.h>\n",
    "\n",
    "#ifdef _OPENMP\n",
    "#include <omp.h>\n",
    "#endif\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int num_devices = omp_get_num_devices();\n",
    "  printf(\"Number of available devices %d\\n\", num_devices);\n",
    "\n",
    "  #pragma omp target\n",
    "  {\n",
    "      if (omp_is_initial_device()) {\n",
    "        printf(\"Running on host\\n\");\n",
    "      } else {\n",
    "        int nteams= omp_get_num_teams();\n",
    "        int nthreads= omp_get_num_threads();\n",
    "        printf(\"Running on device with %d teams in total and %d threads in each team\\n\",nteams,nthreads);\n",
    "      }\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08d8b6",
   "metadata": {},
   "source": [
    "\n",
    "### <span class=\"todo TODO\">TODO</span> OpenACC intro\n",
    "\n",
    "REF:\n",
    "\n",
    "-   <https://www.openacc.org/>\n",
    "-   <https://enccs.github.io/OpenACC-CUDA-beginners/1.02_openacc-introduction/>\n",
    "-   <https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/openacc/basics/>\n",
    "\n",
    "Check if we are using the gpu or the cpu:\n",
    "\n",
    "``` c\n",
    "#include <stdio.h>\n",
    "#include <openacc.h>\n",
    "\n",
    "int main() {\n",
    "  int device_type = acc_get_device_type();\n",
    "\n",
    "  if (device_type == acc_device_nvidia) {\n",
    "    printf(\"Running on an NVIDIA GPU\\n\");\n",
    "  } else if (device_type == acc_device_radeon) {\n",
    "    printf(\"Running on an AMD GPU\\n\");\n",
    "    //} else if (device_type == acc_device_intel_mic) {\n",
    "    //printf(\"Running on an Intel MIC\\n\");\n",
    "  } else if (device_type == acc_device_host) {\n",
    "    printf(\"Running on the host CPU\\n\");\n",
    "  } else {\n",
    "    printf(\"Unknown device type\\n\");\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile as\n",
    "\n",
    "``` bash\n",
    "gcc -fopenacc mycode.c\n",
    "```\n",
    "\n",
    "Simple example:\n",
    "\n",
    "``` c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "  int i;\n",
    "  float a[100], b[100], c[100];\n",
    "\n",
    "  // Initialize arrays\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    a[i] = i;\n",
    "    b[i] = i;\n",
    "  }\n",
    "\n",
    "  // Compute element-wise sum\n",
    "  #pragma acc parallel loop\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    c[i] = a[i] + b[i];\n",
    "  }\n",
    "\n",
    "  // Print result\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    printf(\"%f\\n\", c[i]);\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204d21e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d828fe-2a4b-4e94-b6c3-0ddf9429883b",
   "metadata": {},
   "source": [
    "https://quick-bench.com/\n",
    "\n",
    "# Optimization\n",
    "\n",
    "First of all :\n",
    "\n",
    "> \"Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%.\" - Donald Knuth\n",
    "\n",
    "First, make your program correct, then measure, then optimize.\n",
    "Optimization can be achieved by just using compiler flags, by using\n",
    "already optimized libraries, and sometimes by being careful with our\n",
    "memory usage. Some low level techniques are better left to the compiler.\n",
    "\n",
    "Some tips: <https://news.ycombinator.com/item?id=39564632>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929df8c7",
   "metadata": {},
   "source": [
    "\n",
    "## Compiler flags\n",
    "\n",
    "See : <https://gcc.gnu.org/onlinedocs/gcc/Optimize-Options.html>\n",
    "\n",
    "Some warnings:\n",
    "-   Optimizations enabled by -ffast-math: <https://kristerw.github.io/2021/10/19/fast-math/>\n",
    "-   Beware of fast-math: <https://simonbyrne.github.io/notes/fastmath/>\n",
    "-   Beware of fast-math (Hackernews discussion): <https://news.ycombinator.com/item?id=29201473>\n",
    "\n",
    "Sometimes just using compiler optimization flags with can improve\n",
    "dramatically our code performance. Normally, you want to run your code\n",
    "through several compiler flags to check for speed ups but also for\n",
    "possible uncovered bugs.\n",
    "\n",
    "Compile without (`-O0`) and with optimization (`-O3`) the following code\n",
    "and compare their runtime (this is a very old c-based code but still works):\n",
    "\n",
    "``` c\n",
    "// Credits : Ivan Pulido\n",
    "/* Shows a way to do operations that require a specific order (e.g.,\n",
    " * transpositions) while avoiding cache misses. */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define min( a, b ) ( ((a) < (b)) ? (a) : (b) )\n",
    "\n",
    "int main(){\n",
    "    const int n = 512;\n",
    "    const int csize = 32;\n",
    "    float ***a, ***b;\n",
    "    clock_t cputime1, cputime2;\n",
    "    int i,j,k,ii,jj,kk;\n",
    "\n",
    "    // Allocating memory for array/matrix\n",
    "    a = malloc(n*sizeof(float **));\n",
    "    for (i=0; i<n; i++){\n",
    "        a[i] = malloc(n*sizeof(float*));\n",
    "        for (j=0; j<n; j++)\n",
    "            a[i][j] = malloc(n*sizeof(float));\n",
    "    }\n",
    "    b = malloc(n*sizeof(float **));\n",
    "    for (i=0; i<n; i++){\n",
    "        b[i] = malloc(n*sizeof(float*));\n",
    "        for (j=0; j<n; j++)\n",
    "            b[i][j] = malloc(n*sizeof(float));\n",
    "    }\n",
    "\n",
    "    // Filling matrices with zeros\n",
    "    for(i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j)\n",
    "            for (k=0; k<n; ++k)\n",
    "                a[i][j][k] = 0;\n",
    "    for(i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j)\n",
    "            for (k=0; k<n; ++k)\n",
    "                b[i][j][k] = 0;\n",
    "\n",
    "    // Direct (inefficient) transposition\n",
    "    cputime1 = clock();\n",
    "    for (i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j)\n",
    "            for (k=0; k<n; ++k)\n",
    "                a[i][j][k] = b[k][j][i];\n",
    "    cputime2 = clock() - cputime1;\n",
    "    printf(\"Time for transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    // Transposition using cache-blocking\n",
    "    cputime1 = clock();\n",
    "    for (ii=0; ii<n; ii+=csize)\n",
    "        for (jj=0; jj<n; jj+=csize)\n",
    "            for (kk=0; kk<n; kk+=csize)\n",
    "                for (i=ii; i<min(n,ii+csize-1); ++i)\n",
    "                    for (j=jj; j<min(n,jj+csize-1); ++j)\n",
    "                        for (k=kk; k<min(n,kk+csize-1); ++k)\n",
    "                            a[i][j][k] = b[k][j][i];\n",
    "    cputime2 = clock() - cputime1;\n",
    "    printf(\"Time for transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "When you compile without optimization and execute, you will get\n",
    "something like\n",
    "\n",
    "``` shell\n",
    "gcc -O0 cache_blocking.c\n",
    "./a.out\n",
    "```\n",
    "\n",
    "But, if you use optimization, you can get an important improvement\n",
    "\n",
    "``` shell\n",
    "gcc -O2 cache_blocking.c\n",
    "./a.out\n",
    "```\n",
    "\n",
    "Actually we can compare all optimization levels to check their impact:\n",
    "\n",
    "``` shell\n",
    "for level in 0 1 2 3; do echo \"level: $level\"; gcc -O$level cache_blocking.c; ./a.out; done\n",
    "```\n",
    "\n",
    "Be careful when using `-O3`: it activates some unsafe math optimizations\n",
    "that will not respect IEEE754.\n",
    "\n",
    "### Exercise\n",
    "Vary both the matrix size and the cache size to compute both times. Plot the results on two different 3d plots (in gnuplot use `splot`). Repeat for `-O0` and `-O2`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8c21e",
   "metadata": {},
   "source": [
    "\n",
    "## Memory layout\n",
    "\n",
    "The memory layout should always be taken into account since cache misses\n",
    "will greatly affect a program performance. Always measure with a\n",
    "profiler and if possible with cachegrind in order to detect possible\n",
    "excessive cache misses.\n",
    "\n",
    "See Cache Nightmares: <https://www.youtube.com/watch?v=xFMXIgvlgcY>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696fb01-8f0f-40c4-9d95-e4ba11196fe7",
   "metadata": {},
   "source": [
    "Being nice to the cache comes from the modern cpu architecture, as shown\n",
    "in the next figure\n",
    "\n",
    "L1, L2 and L3 caches. Source\n",
    "<https://medium.com/software-design/why-software-developers-should-care-about-cpu-caches-8da04355bb8a>\n",
    "\n",
    "<img src=\"./cpu-caches.png\" class=centerimg80>\n",
    "\n",
    "The relative bandwidth across the different cpu and momory controllers\n",
    "explain why it is important to have the processed data as much time as\n",
    "possible in the cache\n",
    "\n",
    "Relative bandwidth across different cpu and computer parts. Credit:\n",
    "<https://cs.brown.edu/courses/csci1310/2020/assign/labs/lab4.html\n",
    "\n",
    "<img src=\"./bandwidth.png\" class=centerimg80>\n",
    "\n",
    "Since c/c++ store matrices in row major order, then accesing the memory\n",
    "in the same way will benefit the cache and therefore increases our\n",
    "program performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3250123-615a-45c7-a8a2-08ddf1930420",
   "metadata": {},
   "source": [
    "\n",
    "The following code shows how a simple index change in a matrix operation\n",
    "could have a huge impact on the app performance:\n",
    "\n",
    "```c\n",
    "// Credit Ivan Pulido\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <time.h>\n",
    "\n",
    "int main(){\n",
    "    const int n = 256;\n",
    "    clock_t cputime1, cputime2;\n",
    "    float ***a;\n",
    "    int i,j,k;\n",
    "\n",
    "    // Allocating memory for array/matrix\n",
    "    a = malloc(n*sizeof(float **));\n",
    "    for (i=0; i<n; i++){\n",
    "        a[i] = malloc(n*sizeof(float*));\n",
    "        for (j=0; j<n; j++)\n",
    "            a[i][j] = malloc(n*sizeof(float));\n",
    "    }\n",
    "    cputime1 = clock();\n",
    "    for (k=0; k<n; ++k)\n",
    "        for (j=0; j<n; ++j)\n",
    "            for (i=0; i<n; ++i)\n",
    "                a[i][j][k] = 1.0;\n",
    "    cputime2=clock() - cputime1;\n",
    "    printf(\"Time with fast index inside: %lf\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    cputime1 = clock();\n",
    "    for(i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j)\n",
    "            for (k=0; k<n; ++k)\n",
    "                a[i][j][k] = 2.3;\n",
    "    cputime2=clock() - cputime1;\n",
    "    printf(\"Time with fast index outside: %lf\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    // Clearing memory\n",
    "    for (i=0; i<n; i++){\n",
    "        for (j=0; j<n; j++)\n",
    "            free(a[i][j]);\n",
    "        free(a[i]);\n",
    "    }\n",
    "    free(a);\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "To compile, \n",
    "```shell\n",
    "gcc cache_lines.c\n",
    "./a.out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc2f667-5e41-49e7-acc8-813bd0f9790c",
   "metadata": {},
   "source": [
    "> **Exercise**\n",
    "> - Vary again the matrix size. Is the time difference important for all sizes? plot it.\n",
    "> - Explore the effect of different optimization flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6e64f",
   "metadata": {},
   "source": [
    "\n",
    "### Blocking multiplication\n",
    "\n",
    "See <https://malithjayaweera.com/2020/07/blocked-matrix-multiplication/>\n",
    "\n",
    "Blocking techniques are neat examples that show how being aware of the\n",
    "cache allows you to increase the performance dramatically. For instance,\n",
    "the following code shows a concrete example where a blocking techinque\n",
    "is used to compute the transpose of a matrix, with a important\n",
    "performance advantage:\n",
    "\n",
    "```c\n",
    "// Credits: Ivan Pulido\n",
    "/* Shows a way to do operations that require a specific order (e.g., \n",
    " * transpositions) while avoiding cache misses. */\n",
    "\n",
    "#include <stdio.h>\n",
    "#include <time.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define min( a, b ) ( ((a) < (b)) ? (a) : (b) )\n",
    "\n",
    "int main(){\n",
    "const int n = 512;\n",
    "  const int csize = 32;\n",
    "  float ***a, ***b;\n",
    "  clock_t cputime1, cputime2;\n",
    "  int i,j,k,ii,jj,kk;\n",
    "\n",
    "  // Allocating memory for array/matrix\n",
    "  a = malloc(n*sizeof(float **));\n",
    "  for (i=0; i<n; i++){\n",
    "    a[i] = malloc(n*sizeof(float*));\n",
    "    for (j=0; j<n; j++)\n",
    "      a[i][j] = malloc(n*sizeof(float));\n",
    "  }\n",
    "  b = malloc(n*sizeof(float **));\n",
    "  for (i=0; i<n; i++){\n",
    "    b[i] = malloc(n*sizeof(float*));\n",
    "    for (j=0; j<n; j++)\n",
    "      b[i][j] = malloc(n*sizeof(float));\n",
    "  }\n",
    "\n",
    "  // Filling matrices with zeros\n",
    "  for(i=0; i<n; ++i)\n",
    "    for (j=0; j<n; ++j)\n",
    "      for (k=0; k<n; ++k)\n",
    "        a[i][j][k] = 0;\n",
    "  for(i=0; i<n; ++i)\n",
    "    for (j=0; j<n; ++j)\n",
    "      for (k=0; k<n; ++k)\n",
    "        b[i][j][k] = 0;\n",
    "\n",
    "  // Direct (inefficient) transposition\n",
    "  cputime1 = clock();\n",
    "  for (i=0; i<n; ++i)\n",
    "    for (j=0; j<n; ++j)\n",
    "      for (k=0; k<n; ++k)\n",
    "        a[i][j][k] = b[k][j][i];\n",
    "  cputime2 = clock() - cputime1;\n",
    "  printf(\"Time for transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "  // Transposition using cache-blocking\n",
    "  cputime1 = clock();\n",
    "  for (ii=0; ii<n; ii+=csize)\n",
    "    for (jj=0; jj<n; jj+=csize)\n",
    "      for (kk=0; kk<n; kk+=csize)\n",
    "        for (i=ii; i<min(n,ii+csize-1); ++i)\n",
    "          for (j=jj; j<min(n,jj+csize-1); ++j)\n",
    "            for (k=kk; k<min(n,kk+csize-1); ++k)\n",
    "              a[i][j][k] = b[k][j][i];\n",
    "  cputime2 = clock() - cputime1;\n",
    "  printf(\"Time for transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Compiling and running give the following results\n",
    "\n",
    "``` shell\n",
    "gcc cache_blocking.c\n",
    "./a.out\n",
    "```\n",
    "\n",
    "The second one shows how being cache friendly really helps the\n",
    "performance.\n",
    "\n",
    "> **Exercise**\n",
    "> - Plot the time with the direct and the blocking approaches as functions of size, using `-O2`. What can you conclude?  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c92ab-90e2-4e05-ac16-f4a0f40c3008",
   "metadata": {},
   "source": [
    "## Introduction to gnu parallel\n",
    "Up to now we have been running each program serially. We needed to run several parameters combinations, but we were always running them one after the other even though our computer is multicore. Here we will  learn a tool, called `gnu parallel`, which allows you to run several tasks in parallel.\n",
    "\n",
    "GNU Parallel is a powerful command-line tool that allows you to execute multiple jobs in parallel, making efficient use of multiple CPU cores and dramatically reducing execution time for batch operations.\n",
    "\n",
    "### Basic Syntax\n",
    "\n",
    "```bash\n",
    "parallel [options] command ::: arguments\n",
    "parallel [options] command :::: input-file\n",
    "```\n",
    "\n",
    "### Basic Examples\n",
    "\n",
    "#### Running Multiple Commands in Parallel\n",
    "\n",
    "```bash\n",
    "# Execute multiple independent commands\n",
    "parallel ::: \"echo 'Task 1'\" \"sleep 2; echo 'Task 2'\" \"echo 'Task 3'\"\n",
    "\n",
    "# Download multiple files simultaneously\n",
    "parallel wget ::: \\\n",
    "  https://example.com/file1.zip \\\n",
    "  https://example.com/file2.zip \\\n",
    "  https://example.com/file3.zip\n",
    "```\n",
    "\n",
    "#### Processing Files in Parallel\n",
    "\n",
    "```bash\n",
    "# Compress multiple files\n",
    "parallel gzip ::: *.txt\n",
    "\n",
    "# Convert images with ImageMagick\n",
    "parallel convert {} {.}.jpg ::: *.png\n",
    "\n",
    "# Process log files\n",
    "parallel \"grep 'ERROR' {} > {.}_errors.log\" ::: /var/log/*.log\n",
    "```\n",
    "\n",
    "#### Parameter Combinations\n",
    "\n",
    "```bash\n",
    "# Multiple parameter sets\n",
    "parallel echo \"Processing {1} with {2}\" ::: file1 file2 file3 ::: option-a option-b\n",
    "\n",
    "# Cartesian product of parameters\n",
    "parallel \"convert {1} -resize {2} resized_{1}\" ::: *.jpg ::: 50% 75% 100%\n",
    "\n",
    "# Using input files for parameters\n",
    "echo -e \"param1\\nparam2\\nparam3\" | parallel echo \"Processing: {}\"\n",
    "```\n",
    "\n",
    "### Advanced Examples\n",
    "\n",
    "\n",
    "#### System Administration\n",
    "\n",
    "```bash\n",
    "# Check disk usage on multiple servers\n",
    "parallel -j10 \"ssh {} 'df -h'\" ::: server1 server2 server3\n",
    "\n",
    "# Update packages on multiple systems\n",
    "parallel \"ssh {} 'sudo apt update && sudo apt upgrade -y'\" ::: \\\n",
    "  user@server1 user@server2 user@server3\n",
    "```\n",
    "\n",
    "#### Data Processing\n",
    "\n",
    "```bash\n",
    "# Process CSV files with different tools\n",
    "parallel \"cut -d',' -f{2} {1} > {1.}_{2}.csv\" ::: data*.csv ::: 1 2 3 4\n",
    "\n",
    "# Run analysis scripts with different parameters\n",
    "parallel \"./analyze.py --input {} --threshold {2}\" ::: *.dat ::: 0.1 0.5 0.9\n",
    "```\n",
    "\n",
    "#### Batch Image/Video Processing\n",
    "\n",
    "```bash\n",
    "# Resize images to multiple sizes\n",
    "parallel \"convert {} -resize {2}x{2} {1/.}_{2}.{1/}\" ::: *.jpg ::: 100 200 300\n",
    "\n",
    "# Extract video thumbnails at different timestamps\n",
    "parallel \"ffmpeg -i video.mp4 -ss {1} -vframes 1 thumb_{1}.jpg\" ::: 00:10 00:30 01:00\n",
    "```\n",
    "\n",
    "### Key Options and Features\n",
    "\n",
    "#### Controlling Parallelism\n",
    "\n",
    "```bash\n",
    "# Limit number of parallel jobs\n",
    "parallel -j4 command ::: args\n",
    "\n",
    "# Use all available CPU cores\n",
    "parallel -j0 command ::: args\n",
    "\n",
    "# Run jobs sequentially (useful for debugging)\n",
    "parallel -j1 command ::: args\n",
    "```\n",
    "\n",
    "#### Progress and Monitoring\n",
    "\n",
    "```bash\n",
    "# Show progress bar\n",
    "parallel --progress command ::: args\n",
    "\n",
    "# Show which command is running\n",
    "parallel --verbose command ::: args\n",
    "\n",
    "# Combine both\n",
    "parallel --progress --verbose command ::: args\n",
    "```\n",
    "\n",
    "#### Error Handling\n",
    "\n",
    "```bash\n",
    "# Continue on errors, collect failed jobs\n",
    "parallel --keep-order --joblog logfile command ::: args\n",
    "\n",
    "# Retry failed jobs\n",
    "parallel --retry-failed --joblog logfile\n",
    "\n",
    "# Resume interrupted jobs\n",
    "parallel --resume --joblog logfile command ::: args\n",
    "```\n",
    "\n",
    "### Comparison with Other Tools\n",
    "\n",
    "#### vs. xargs -P\n",
    "\n",
    "**GNU Parallel Advantages:**\n",
    "- More intuitive syntax for complex parameter combinations\n",
    "- Better job control and monitoring\n",
    "- Built-in resume functionality\n",
    "- Superior error handling and logging\n",
    "- More flexible input methods\n",
    "\n",
    "```bash\n",
    "# xargs approach\n",
    "echo -e \"file1\\nfile2\\nfile3\" | xargs -P4 -I{} gzip {}\n",
    "\n",
    "# GNU Parallel approach (cleaner)\n",
    "parallel gzip ::: file1 file2 file3\n",
    "```\n",
    "\n",
    "#### vs. Bash Background Jobs (&)\n",
    "\n",
    "**GNU Parallel Advantages:**\n",
    "- Automatic job limiting (prevents system overload)\n",
    "- Built-in progress monitoring\n",
    "- Better error collection and handling\n",
    "- Automatic cleanup of completed jobs\n",
    "\n",
    "```bash\n",
    "# Bash background jobs (manual management)\n",
    "for file in *.txt; do\n",
    "    gzip \"$file\" &\n",
    "done\n",
    "wait\n",
    "\n",
    "# GNU Parallel (automatic management)\n",
    "parallel gzip ::: *.txt\n",
    "```\n",
    "\n",
    "#### vs. Make -j\n",
    "\n",
    "**GNU Parallel Advantages:**\n",
    "- No need to write Makefiles for simple parallel tasks\n",
    "- Dynamic job creation from command-line arguments\n",
    "- Better suited for data processing pipelines\n",
    "- More flexible parameter handling\n",
    "\n",
    "#### vs. Custom Shell Scripts\n",
    "\n",
    "**GNU Parallel Advantages:**\n",
    "- No need to write custom parallel execution logic\n",
    "- Built-in job queuing and resource management\n",
    "- Extensive logging and monitoring capabilities\n",
    "- Cross-platform compatibility\n",
    "\n",
    "### Key Advantages of GNU Parallel\n",
    "\n",
    "1. **Simplicity**: Easy to parallelize existing sequential commands\n",
    "2. **Flexibility**: Handles complex parameter combinations effortlessly\n",
    "3. **Resource Management**: Automatically manages CPU and memory usage\n",
    "4. **Robustness**: Built-in error handling, logging, and resume capabilities\n",
    "5. **Monitoring**: Real-time progress tracking and verbose output options\n",
    "6. **Scalability**: Can distribute jobs across multiple machines\n",
    "7. **Compatibility**: Works with any command-line tool\n",
    "8. **Performance**: Significant speedup for I/O and CPU-intensive tasks\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Start with low parallelism** (`-j2` or `-j4`) and increase gradually\n",
    "2. **Use `--dry-run`** to test commands before execution\n",
    "3. **Monitor system resources** to avoid overloading\n",
    "4. **Use `--joblog`** for long-running tasks\n",
    "5. **Test with small datasets** before processing large amounts of data\n",
    "6. **Consider I/O bottlenecks** when setting parallelism levels\n",
    "\n",
    "### Performance Example\n",
    "\n",
    "Sequential processing of 100 files:\n",
    "```bash\n",
    "time for f in file*.txt; do gzip \"$f\"; done\n",
    "# Real: 2m30s\n",
    "```\n",
    "\n",
    "Parallel processing with GNU Parallel:\n",
    "```bash\n",
    "time parallel gzip ::: file*.txt\n",
    "# Real: 0m25s (10x speedup on 8-core system)\n",
    "```\n",
    "\n",
    "GNU Parallel is an essential tool for anyone working with batch processing, data analysis, or system administration tasks where parallel execution can provide significant performance improvements.\n",
    "\n",
    "> **Exercise**\n",
    "> Plot the Run the cache lines examples for several optimization levels and matrix sizes. Run the simulations in parallel using gnu parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9e428",
   "metadata": {},
   "source": [
    "\n",
    "## How to model simple matrices?\n",
    "\n",
    "It depends greatly from the problem. But for the typical applications of\n",
    "vector and matrix computations, we can expect that homogeneous and\n",
    "contiguous arrays are the data structures to go. In that case, it is\n",
    "advisable to use `std::vector` as the go to data struct since it is as\n",
    "efficient as an primitive arra, handles automatically the dynamic memory\n",
    "in the heap, and plays nice with the C++ STL.\n",
    "\n",
    "A primitive static array is limited by the stack. Play with the values\n",
    "of M and N in the following code until you get a seg fault just by\n",
    "running the code.\n",
    "\n",
    "```c++\n",
    "#include <iostream>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    const int M = 7000;\n",
    "    const int N = 500;\n",
    "    double data[M][N] = {{0.0}};\n",
    "\n",
    "    std::cout << data[M/2][N/2] << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Is the maximum size related somehow with the output of the command\n",
    "`ulimit -s` ?\n",
    "\n",
    "To be able to use more memory, you could better use dynamic memory with\n",
    "primitive arrays, but you will have to manage the `new/delete` parts in\n",
    "your code. Check that foloowing code does not die on the same sizes as\n",
    "the previous one\n",
    "\n",
    "```c++\n",
    "#include <iostream>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    const int M = 7000;\n",
    "    const int N = 500;\n",
    "\n",
    "    double **data = nullptr;\n",
    "    data = new double *[M];\n",
    "    for (int ii = 0; ii < M; ++ii){\n",
    "        data[ii] = new double [N];\n",
    "    }\n",
    "\n",
    "    std::cout << data[M/2][N/2] << std::endl;\n",
    "\n",
    "    for (int ii = 0; ii < M; ++ii){\n",
    "        delete [] data[ii];\n",
    "    }\n",
    "    delete [] data;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "But be careful, memory obtained using double/multiple pointers is not\n",
    "guaranteed to be contiguous, so it is much better to just ask for a large\n",
    "one-dimensional array and just model the 2D shape with smart indexes\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  const int M = 7000;\n",
    "  const int N = 500;\n",
    "\n",
    "  double *data = nullptr;\n",
    "  data = new double [M*N];\n",
    "\n",
    "  // [id][jd] -> id*N + jd\n",
    "  std::cout << data[M*N/2 + N/2] << std::endl;\n",
    "\n",
    "  delete [] data;\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "This is an example of a 2d array actually stored as a 1d array in memory:\n",
    "<img src=\"fig/2d-array-memory.png\" class=centerimg80>\n",
    "\n",
    "And this is the illustration of the mapping that allows us to go to/from different dimensions:\n",
    "<img src=\"fig/1d-2d-mapping.png\" class=centerimg80>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Finally, to avoid managing manually the memory, it is much better to use\n",
    "`std::vector`,\n",
    "\n",
    "``` c++\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    const int M = 7000;\n",
    "    const int N = 500;\n",
    "\n",
    "    std::vector<double> data;\n",
    "    data.resize(M*N);\n",
    "\n",
    "    // [id][jd] -> id*N + jd\n",
    "    std::cout << data[M*N/2 + N/2] << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "> **Exercise**\n",
    "> - Test the stack program to find the maximum matrix size you can use\n",
    "> - Do the same for the last one using a vector, what is the maximum size you can reserve? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c452e",
   "metadata": {},
   "source": [
    "\n",
    "## Using scientific libraries\n",
    "\n",
    "Scientific libraries are written by people with deep knowledge of the\n",
    "computer and algorithms internals , therefore saving developer time and\n",
    "resources. One should always try to use a established scientific library\n",
    "instead of writting everything from scratch. Sometimes even automatic\n",
    "parallelization comes for free. Examples are the Intel or AMD math\n",
    "libraries, the Cuda toolkit for programming on nvidia cards, and so on.\n",
    "\n",
    "The following examples shows the time taken to transpose a matrix using\n",
    "a traditional approach, a blocking approach , and the eigen c++ library, giving the following results:\n",
    "\n",
    "``` bash\n",
    "Time for memory allocation: 0.315618\n",
    "Time for filling: 3.607817\n",
    "Time for direct transposition: 2.870691\n",
    "Time for blocked transposition: 0.380954\n",
    "Time for transposition with eigen: 0.031344\n",
    "Time for transposition with copy in eigen: 0.310033\n",
    "Time for transposition with full copy in eigen: 3.339495\n",
    "-2999.9\n",
    "-2999.9\n",
    "15200.1\n",
    "15200.1\n",
    "15200.1\n",
    "```\n",
    "\n",
    "This shows, for instance, that eigen in some cases is not even computing\n",
    "the transpose but just creating an expression to access the original\n",
    "matrix, hence the huge speeedup and also the slow down when fully\n",
    "creating a copy. \n",
    "\n",
    "The code used is\n",
    "```c++\n",
    "\n",
    "#include <cstdio>\n",
    "#include <ctime>\n",
    "#include <cstdlib>\n",
    "#include <cassert>\n",
    "#include <iostream>\n",
    "#include <Eigen/Dense>\n",
    "\n",
    "#define min( a, b ) ( ((a) < (b)) ? (a) : (b) )\n",
    "\n",
    "bool verify_transpose(float ** m1, float ** m2, int n);\n",
    "template <class T, class U>\n",
    "bool verify_transpose(T & m1, U & m2, int n);\n",
    "\n",
    "int main(){\n",
    "    const int n = 12000;\n",
    "    const int csize = 8;\n",
    "    float **a, **b;\n",
    "    clock_t cputime1, cputime2;\n",
    "    int i,j,k,ii,jj,kk;\n",
    "\n",
    "    // Allocating memory for array/matrix\n",
    "    cputime1 = clock();\n",
    "    a = new float * [n];\n",
    "    for (i=0; i<n; i++){\n",
    "        a[i] = new float [n];\n",
    "    }\n",
    "    b = new float * [n];\n",
    "    for (i=0; i<n; i++){\n",
    "        b[i] = new float[n];\n",
    "    }\n",
    "    // eigen matrices\n",
    "    Eigen::MatrixXf M2 = Eigen::MatrixXf::Constant(n, n, 0.0);\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for memory allocation: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    // Filling matrices with zeros\n",
    "    cputime1 = clock();\n",
    "    for(i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j) {\n",
    "            a[i][j] = 2.3*i + j + 0.1;\n",
    "        }\n",
    "    for(i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j) {\n",
    "            b[i][j] = -2.3*i + 1.8*j + 0.1;\n",
    "            M2(i, j) = 2.3*i + j + 0.1;\n",
    "        }\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for filling: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    // Direct (inefficient) transposition\n",
    "    cputime1 = clock();\n",
    "    for (i=0; i<n; ++i)\n",
    "        for (j=0; j<n; ++j)\n",
    "            a[i][j] = b[j][i];\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for direct transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "    assert(verify_transpose(a, b, n));\n",
    "\n",
    "    // Transposition using cache-blocking\n",
    "    cputime1 = clock();\n",
    "    for (ii=0; ii<n; ii+=csize)\n",
    "        for (jj=0; jj<n; jj+=csize)\n",
    "            for (i=ii; i<min(n,ii+csize-1); ++i)\n",
    "                for (j=jj; j<min(n,jj+csize-1); ++j)\n",
    "                    a[i][j] = b[j][i];\n",
    "    assert(verify_transpose(a, b, n));\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for blocked transposition: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "\n",
    "    // eigen\n",
    "    cputime1 = clock();\n",
    "    //M1.noalias() = M2.transpose();\n",
    "    auto M1{M2.transpose()};\n",
    "    assert(verify_transpose(M1, M2, n));\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for transposition with eigen: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "    cputime1 = clock();\n",
    "    auto M3 = M2.transpose();\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for transposition with copy in eigen: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "    cputime1 = clock();\n",
    "    Eigen::MatrixXf M4 = M2.transpose();\n",
    "    cputime2 = clock() - cputime1;\n",
    "    std::printf(\"Time for transposition with full copy in eigen: %f\\n\", ((double)cputime2)/CLOCKS_PER_SEC);\n",
    "    assert(verify_transpose(M1, M2, n));\n",
    "    assert(verify_transpose(M3, M2, n));\n",
    "    assert(verify_transpose(M4, M2, n));\n",
    "\n",
    "    // use data\n",
    "    M1(n/4, n/2) = 0.9876;\n",
    "    M2(n/2, n/4)= 9.88;\n",
    "    std::cout << a[n/2][n/2] << std::endl;\n",
    "    std::cout << b[n/2][n/2] << std::endl;\n",
    "    std::cout << M1(n/2,n/3) << std::endl;\n",
    "    std::cout << M2(n/3,n/2) << std::endl;\n",
    "    std::cout << M3(n/2,n/3) << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "bool verify_transpose(float ** m1, float ** m2, int n)\n",
    "{\n",
    "    for (int ii = 0; ii < n; ++ii) {\n",
    "        for (int jj = 0; jj < n; ++jj) {\n",
    "            if (m1[ii][jj] != m2[jj][ii]) {\n",
    "                return false;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return true;\n",
    "}\n",
    "\n",
    "template <class T, class U>\n",
    "bool verify_transpose(T & m1, U & m2, int n)\n",
    "{\n",
    "    for (int ii = 0; ii < n; ++ii) {\n",
    "        for (int jj = 0; jj < n; ++jj) {\n",
    "            if (m1(ii, jj) != m2(jj, ii)) {\n",
    "                return false;\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    return true;\n",
    "\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57948af",
   "metadata": {},
   "source": [
    "\n",
    "## Other optimization techniques\n",
    "\n",
    "There are other techniques sometimes applied in very speficic\n",
    "situations, like\n",
    "\n",
    "-   Loop interchanges\n",
    "-   Loop unrolling\n",
    "\n",
    "``` c++\n",
    "for(int ii = 0; ii < n; ii++) {\n",
    "    array[ii] = 2*ii;\n",
    "}\n",
    "```\n",
    "\n",
    "``` c++\n",
    "for(int ii = 0; ii < n; ii += 3) {\n",
    "    array[ii] = 2*ii;\n",
    "    array[ii+1] = 2*(ii+1);\n",
    "    array[ii+2] = 2*(ii+2);\n",
    "}\n",
    "```\n",
    "\n",
    "-   Loop Fusion/Fision\n",
    "-   Prefetching\n",
    "-   Floating point division\n",
    "-   Vectorization\n",
    "-   etc\n",
    "\n",
    "In general those techniques can applied after a careful determination\n",
    "that they are really needed, and sometimes the compilers already apply\n",
    "them at some optimization levels. Therefore it is advisable to focus on\n",
    "the clarity of the program first and let the compiler do its job.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede67739",
   "metadata": {},
   "source": [
    "\n",
    "## Other resources\n",
    "\n",
    "-   <https://www.agner.org/optimize/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d2063",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11e08780-d5a6-444c-9cd6-dafb58471036",
   "metadata": {},
   "source": [
    "# Ref\n",
    "\n",
    "-   <https://codeconfessions.substack.com/p/gpu-computing>\n",
    "-   <https://libocca.org/#/guide/user-guide/introduction>\n",
    "-   <https://news.ycombinator.com/item?id=36304143>\n",
    "-   <https://news.ycombinator.com/item?id=36210599>\n",
    "\n",
    "<https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-1/>\n",
    "\n",
    "https://www.youtube.com/watch?v=h9Z4oGN89MU : How does a gpu work\n",
    "\n",
    "https://news.ycombinator.com/item?id=42042016"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e15c63-9a0d-42e0-a42b-87a008576e94",
   "metadata": {},
   "source": [
    "\n",
    "### Environment setup for cuda\n",
    "\n",
    "Here I show two ways to setup the dev environment. One is based on a\n",
    "local computer with a graphics card, and the other using google collab.\n",
    "\n",
    "1.  Local environment\n",
    "\n",
    "    Here we will setup a computer which has an Nvidia Quadro P1000 card.\n",
    "    You need to install both the driver and the cuda toolkit (the later\n",
    "    better to be installed as a part of the nvidia sdk)\n",
    "\n",
    "    -   Driver download for quadro P1000:\n",
    "        <https://www.nvidia.com/Download/driverResults.aspx/204639/en-us/>\n",
    "    -   Nvidia sdk: <https://developer.nvidia.com/hpc-sdk-downloads>\n",
    "        -   Nvidia singularity: This is the recommended way. The image\n",
    "            is built at\n",
    "            /packages/nvhpc<sub>23</sub>.3<sub>devel</sub>.sif. More\n",
    "            instructions at\n",
    "            <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc>\n",
    "\n",
    "            1.  Accesing a shell inside the container but with\n",
    "                visibility to all user account files:\n",
    "\n",
    "                ``` bash\n",
    "                singularity shell --nv /packages/nvhpc_23.3_devel.sif\n",
    "                ```\n",
    "\n",
    "            2.  Compiling\n",
    "\n",
    "                ``` bash\n",
    "                singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "                ```\n",
    "\n",
    "            3.  Executing with nvprof\n",
    "\n",
    "                ``` bash\n",
    "                singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "                ```\n",
    "\n",
    "        -   Local module: Load the nvidia sdk (sala2):\n",
    "\n",
    "            ``` bash\n",
    "            module load /packages/nvidia/hpc_sdk/modulefiles/nvhpc/23.3\n",
    "            ```\n",
    "\n",
    "            Compile as\n",
    "\n",
    "            ``` bash\n",
    "            nvc++  -std=c++17 -o offload.x offload.cpp\n",
    "            ```\n",
    "\n",
    "        -   The docker container is installed. Unfortunately it does not\n",
    "            run since the device compute capability is not enough\n",
    "\n",
    "            ``` bash\n",
    "            docker run --gpus all -it --rm nvcr.io/nvidia/nvhpc:23.3-devel-cuda_multi-ubuntu20.04\n",
    "            docker: Error response from daemon: could not select device driver \"\" with capabilities: [[gpu]].\n",
    "            ```\n",
    "\n",
    "            More info about container:\n",
    "            <https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc>\n",
    "\n",
    "2.  Google collab\n",
    "\n",
    "    Open a collab notebook, go to runtime, change runtime type, hardware\n",
    "    accelerator -\\> GPU, GPU type -\\> T4, Save. The you will have a\n",
    "    runtime with a T4 card, for free. If you want an even better card,\n",
    "    you can pay for collab pro.\n",
    "\n",
    "    Inside the notebook, you can run commands with the prefix `!` to run\n",
    "    then as in a console. For instance, to get the device properties,\n",
    "    you can run\n",
    "\n",
    "    ``` bash\n",
    "    !nvidia-smi\n",
    "    ```\n",
    "\n",
    "    to get something like\n",
    "\n",
    "    ``` bash\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
    "    |-------------------------------+----------------------+----------------------+\n",
    "    | GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "    | Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "    |                               |                      |               MIG M. |\n",
    "    |===============================+======================+======================|\n",
    "    |   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
    "    | N/A   44C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
    "    |                               |                      |                  N/A |\n",
    "    +-------------------------------+----------------------+----------------------+\n",
    "\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    | Processes:                                                                  |\n",
    "    |  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "    |        ID   ID                                                   Usage      |\n",
    "    |=============================================================================|\n",
    "    |  No running processes found                                                 |\n",
    "    +-----------------------------------------------------------------------------+\n",
    "    ```\n",
    "\n",
    "    To create local files, like `filename.cu`, use the magic\n",
    "    `%%writefile\n",
    "    filename.cu` at the beginning of the cell and then put the file\n",
    "    contents in the same cell.\n",
    "\n",
    "    Finally, to compile and run just execute the following\n",
    "\n",
    "    ``` bash\n",
    "    !nvcc filename.cu -o name.x\n",
    "    !nvprof ./name.x\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35526083-57f8-420c-8c80-a32fc8f3e409",
   "metadata": {},
   "source": [
    "\n",
    "### <span class=\"todo TODO\">TODO</span> Cuda intro\n",
    "\n",
    "REF <https://en.wikipedia.org/wiki/CUDA?useskin=vector> Tutorial1:\n",
    "<https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/>\n",
    "Tutorial2\n",
    "<https://developer.nvidia.com/blog/even-easier-introduction-cuda/>\n",
    "\n",
    "1.  Tutorial 1\n",
    "\n",
    "    <https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/>\n",
    "    Example in c\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    gcc example_01.c\n",
    "    ```\n",
    "\n",
    "    Now the same but in cuda:\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    nvcc example_01.cu\n",
    "    ```\n",
    "\n",
    "    Execution will show errors, due to the fact that the code is NOT\n",
    "    running on the device.\n",
    "\n",
    "    We need to allocate memory on it(`cudaMalloc` and `cudaFree`), and\n",
    "    trasfer data to and from it (`cudaMemCopy`).\n",
    "\n",
    "2.  Tutorial 2\n",
    "\n",
    "    <https://developer.nvidia.com/blog/even-easier-introduction-cuda/>\n",
    "\n",
    "    ``` cpp\n",
    "    #include <iostream>\n",
    "    #include <math.h>\n",
    "\n",
    "    // function to add the elements of two arrays\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      for (int i = 0; i < n; i++)\n",
    "          y[i] = x[i] + y[i];\n",
    "    }\n",
    "\n",
    "    int main(void)\n",
    "    {\n",
    "      int N = 1<<20; // 1M elements\n",
    "\n",
    "      float *x = new float[N];\n",
    "      float *y = new float[N];\n",
    "\n",
    "      // initialize x and y arrays on the host\n",
    "      for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "      }\n",
    "\n",
    "      // Run kernel on 1M elements on the CPU\n",
    "      add(N, x, y);\n",
    "\n",
    "      // Check for errors (all values should be 3.0f)\n",
    "      float maxError = 0.0f;\n",
    "      for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "      std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "      // Free memory\n",
    "      delete [] x;\n",
    "      delete [] y;\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Compile as\n",
    "\n",
    "    ``` bash\n",
    "    g++ -g -std=c++17 cuda_01.cpp\n",
    "    ```\n",
    "\n",
    "    Cuda example\n",
    "\n",
    "    ``` cuda\n",
    "    #include <iostream>\n",
    "    #include <math.h>\n",
    "    // Kernel function to add the elements of two arrays\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      for (int i = 0; i < n; i++)\n",
    "        y[i] = x[i] + y[i];\n",
    "    }\n",
    "\n",
    "    int main(void)\n",
    "    {\n",
    "      int N = 1<<20;\n",
    "      float *x, *y;\n",
    "\n",
    "      // Allocate Unified Memory â€“ accessible from CPU or GPU\n",
    "      cudaMallocManaged(&x, N*sizeof(float));\n",
    "      cudaMallocManaged(&y, N*sizeof(float));\n",
    "\n",
    "      // initialize x and y arrays on the host\n",
    "      for (int i = 0; i < N; i++) {\n",
    "        x[i] = 1.0f;\n",
    "        y[i] = 2.0f;\n",
    "      }\n",
    "\n",
    "      // Run kernel on 1M elements on the GPU\n",
    "      add<<<1, 1>>>(N, x, y);\n",
    "\n",
    "      // Wait for GPU to finish before accessing on host\n",
    "      cudaDeviceSynchronize();\n",
    "\n",
    "      // Check for errors (all values should be 3.0f)\n",
    "      float maxError = 0.0f;\n",
    "      for (int i = 0; i < N; i++)\n",
    "        maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
    "      std::cout << \"Max error: \" << maxError << std::endl;\n",
    "\n",
    "      // Free memory\n",
    "      cudaFree(x);\n",
    "      cudaFree(y);\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    To compile, use `nvc++`.\n",
    "\n",
    "    If you have a singularity container with the nvidia sdk, you can\n",
    "    just run the following\n",
    "\n",
    "    ``` bash\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif nvc++ -g cuda_02.cu\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif ./a.out\n",
    "    singularity exec --nv /packages/nvhpc_23.3_devel.sif nvprof ./a.out\n",
    "    ```\n",
    "\n",
    "    and get something like\n",
    "\n",
    "    ``` bash\n",
    "    ==16094== NVPROF is profiling process 16094, command: ./a.out\n",
    "    Max error: 0\n",
    "    ==16094== Profiling application: ./a.out\n",
    "    ==16094== Profiling result:\n",
    "                Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "     GPU activities:  100.00%  2.54774s         1  2.54774s  2.54774s  2.54774s  add(int, float*, float*)\n",
    "          API calls:   93.27%  2.54776s         1  2.54776s  2.54776s  2.54776s  cudaDeviceSynchronize\n",
    "                        6.71%  183.20ms         2  91.602ms  20.540us  183.18ms  cudaMallocManaged\n",
    "                        0.02%  468.25us         2  234.13us  216.27us  251.98us  cudaFree\n",
    "                        0.01%  213.75us       101  2.1160us     141ns  150.11us  cuDeviceGetAttribute\n",
    "                        0.00%  32.127us         1  32.127us  32.127us  32.127us  cudaLaunchKernel\n",
    "                        0.00%  22.239us         1  22.239us  22.239us  22.239us  cuDeviceGetName\n",
    "                        0.00%  6.1330us         1  6.1330us  6.1330us  6.1330us  cuDeviceGetPCIBusId\n",
    "                        0.00%  1.5730us         3     524ns     197ns  1.1650us  cuDeviceGetCount\n",
    "                        0.00%     808ns         2     404ns     141ns     667ns  cuDeviceGet\n",
    "                        0.00%     530ns         1     530ns     530ns     530ns  cuDeviceTotalMem\n",
    "                        0.00%     243ns         1     243ns     243ns     243ns  cuDeviceGetUuid\n",
    "\n",
    "    ==16094== Unified Memory profiling result:\n",
    "    Device \"Quadro P1000 (0)\"\n",
    "       Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "          48  170.67KB  4.0000KB  0.9961MB  8.000000MB  735.2380us  Host To Device\n",
    "          24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.3770us  Device To Host\n",
    "          24         -         -         -           -  2.855987ms  Gpu page fault groups\n",
    "    Total CPU Page faults: 36\n",
    "    ```\n",
    "\n",
    "    You can also run it on google collab, where you will have an nvidia\n",
    "    T4 card available for free (after changing the runtime), with the\n",
    "    following typical output\n",
    "\n",
    "    ``` bash\n",
    "    ==18853== NVPROF is profiling process 18853, command: ./cuda_02.x\n",
    "    Max error: 0\n",
    "    ==18853== Profiling application: ./cuda_02.x\n",
    "    ==18853== Profiling result:\n",
    "                Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "     GPU activities:  100.00%  108.83ms         1  108.83ms  108.83ms  108.83ms  add(int, float*, float*)\n",
    "          API calls:   72.48%  290.34ms         2  145.17ms  36.191us  290.31ms  cudaMallocManaged\n",
    "                       27.17%  108.84ms         1  108.84ms  108.84ms  108.84ms  cudaDeviceSynchronize\n",
    "                        0.28%  1.1298ms         2  564.90us  537.96us  591.84us  cudaFree\n",
    "                        0.05%  182.13us       101  1.8030us     264ns  75.268us  cuDeviceGetAttribute\n",
    "                        0.01%  48.553us         1  48.553us  48.553us  48.553us  cudaLaunchKernel\n",
    "                        0.01%  28.488us         1  28.488us  28.488us  28.488us  cuDeviceGetName\n",
    "                        0.00%  8.6520us         1  8.6520us  8.6520us  8.6520us  cuDeviceGetPCIBusId\n",
    "                        0.00%  2.3140us         3     771ns     328ns  1.6230us  cuDeviceGetCount\n",
    "                        0.00%     919ns         2     459ns     315ns     604ns  cuDeviceGet\n",
    "                        0.00%     580ns         1     580ns     580ns     580ns  cuDeviceTotalMem\n",
    "                        0.00%     532ns         1     532ns     532ns     532ns  cuModuleGetLoadingMode\n",
    "                        0.00%     382ns         1     382ns     382ns     382ns  cuDeviceGetUuid\n",
    "\n",
    "    ==18853== Unified Memory profiling result:\n",
    "    Device \"Tesla T4 (0)\"\n",
    "       Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "          48  170.67KB  4.0000KB  0.9961MB  8.000000MB  809.9640us  Host To Device\n",
    "          24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.6320us  Device To Host\n",
    "          12         -         -         -           -  2.564287ms  Gpu page fault groups\n",
    "    Total CPU Page faults: 36\n",
    "    ```\n",
    "\n",
    "    If you increase just the number of threads to 256 (check the change\n",
    "    in `<<<...>>>`), and split correctly the work using the cuda vars\n",
    "    `threadIdx.x` (thread id in the block) and `blockDim.x` (number of\n",
    "    threads in the block), as shown,\n",
    "\n",
    "    ``` cuda\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      int index = threadIdx.x;\n",
    "      int stride = blockDim.x;\n",
    "      for (int i = index; i < n; i += stride)\n",
    "          y[i] = x[i] + y[i];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    ``` cuda\n",
    "    // Run kernel on 1M elements on the GPU\n",
    "      add<<<1, 256>>>(N, x, y);\n",
    "    ```\n",
    "\n",
    "    then you get the following output\n",
    "\n",
    "    -   Quadro P1000 : From 2.5 secs to 0.022 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==21739== NVPROF is profiling process 21739, command: ./a.out\n",
    "        Max error: 0\n",
    "        ==21739== Profiling application: ./a.out\n",
    "        ==21739== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  21.978ms         1  21.978ms  21.978ms  21.978ms  add(int, float*, float*)\n",
    "              API calls:   87.86%  164.24ms         2  82.118ms  12.398us  164.22ms  cudaMallocManaged\n",
    "                           11.76%  21.980ms         1  21.980ms  21.980ms  21.980ms  cudaDeviceSynchronize\n",
    "                            0.24%  457.32us         2  228.66us  177.89us  279.43us  cudaFree\n",
    "                            0.11%  206.80us       101  2.0470us     128ns  144.81us  cuDeviceGetAttribute\n",
    "                            0.02%  29.041us         1  29.041us  29.041us  29.041us  cudaLaunchKernel\n",
    "                            0.01%  20.149us         1  20.149us  20.149us  20.149us  cuDeviceGetName\n",
    "                            0.00%  5.5860us         1  5.5860us  5.5860us  5.5860us  cuDeviceGetPCIBusId\n",
    "                            0.00%  2.1000us         3     700ns     277ns     958ns  cuDeviceGetCount\n",
    "                            0.00%     952ns         2     476ns     330ns     622ns  cuDeviceGet\n",
    "                            0.00%     391ns         1     391ns     391ns     391ns  cuDeviceTotalMem\n",
    "                            0.00%     259ns         1     259ns     259ns     259ns  cuDeviceGetUuid\n",
    "\n",
    "        ==21739== Unified Memory profiling result:\n",
    "        Device \"Quadro P1000 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              48  170.67KB  4.0000KB  0.9961MB  8.000000MB  734.5940us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  338.5950us  Device To Host\n",
    "              24         -         -         -           -  1.764587ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    -   Tesla T4: From 0.108 secs to 0.004 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==21448== NVPROF is profiling process 21448, command: ./cuda_03.x\n",
    "        Max error: 0\n",
    "        ==21448== Profiling application: ./cuda_03.x\n",
    "        ==21448== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  3.7978ms         1  3.7978ms  3.7978ms  3.7978ms  add(int, float*, float*)\n",
    "              API calls:   98.24%  291.22ms         2  145.61ms  73.005us  291.15ms  cudaMallocManaged\n",
    "                            1.28%  3.8044ms         1  3.8044ms  3.8044ms  3.8044ms  cudaDeviceSynchronize\n",
    "                            0.36%  1.0699ms         2  534.95us  512.29us  557.62us  cudaFree\n",
    "                            0.08%  222.64us       101  2.2040us     174ns  102.62us  cuDeviceGetAttribute\n",
    "                            0.02%  62.588us         1  62.588us  62.588us  62.588us  cudaLaunchKernel\n",
    "                            0.02%  44.725us         1  44.725us  44.725us  44.725us  cuDeviceGetName\n",
    "                            0.00%  8.1290us         1  8.1290us  8.1290us  8.1290us  cuDeviceGetPCIBusId\n",
    "                            0.00%  3.2970us         3  1.0990us     266ns  2.6840us  cuDeviceGetCount\n",
    "                            0.00%  1.7320us         2     866ns     352ns  1.3800us  cuDeviceGet\n",
    "                            0.00%     632ns         1     632ns     632ns     632ns  cuDeviceTotalMem\n",
    "                            0.00%     549ns         1     549ns     549ns     549ns  cuModuleGetLoadingMode\n",
    "                            0.00%     377ns         1     377ns     377ns     377ns  cuDeviceGetUuid\n",
    "\n",
    "        ==21448== Unified Memory profiling result:\n",
    "        Device \"Tesla T4 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              48  170.67KB  4.0000KB  0.9961MB  8.000000MB  825.8720us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  360.3130us  Device To Host\n",
    "              13         -         -         -           -  2.951606ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    Cuda devices group parallel processors into Streaming\n",
    "    Multiprocessors (SM), and each of them can run several threads in\n",
    "    parallel. In our case, by using the command `deviceQuery` (for the\n",
    "    QuadroP1000 system it is at\n",
    "    `/opt/cuda/extras/demo_suite/deviceQuery`), we get\n",
    "\n",
    "    -   Quadro P1000: 5 SM, 128 threads/SM\n",
    "    -   Tesla T4: 32 SM, 128 threads/SM\n",
    "\n",
    "    So the ideal number of threads changes per card, and we will compute\n",
    "    as\n",
    "\n",
    "    ``` cpp\n",
    "    int blockSize = 128;\n",
    "    int numBlocks = (N + blockSize - 1) / blockSize; // what if N is not divisible by blocksize?\n",
    "    add<<<numBlocks, blockSize>>>(N, x, y);\n",
    "    ```\n",
    "\n",
    "    Notice that you can also compute this constant by using the follow\n",
    "    code (generated by bard.google.com)\n",
    "\n",
    "    ``` cpp\n",
    "    // Get the number of threads per multiprocessor.\n",
    "    int threadsPerMultiprocessor;\n",
    "    cudaError_t err = cudaDeviceGetAttribute(&threadsPerMultiprocessor, cudaDevAttrMaxThreadsPerMultiprocessor, device);\n",
    "    if (err != cudaSuccess) {\n",
    "      // Handle error.\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    The kernel will now become\n",
    "\n",
    "    ``` cpp\n",
    "    __global__\n",
    "    void add(int n, float *x, float *y)\n",
    "    {\n",
    "      int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "      int stride = blockDim.x * gridDim.x;\n",
    "      for (int i = index; i < n; i += stride)\n",
    "        y[i] = x[i] + y[i];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "    based on the job distribution done by the tutorial\n",
    "    <https://developer-blogs.nvidia.com/wp-content/uploads/2017/01/cuda_indexing.png>\n",
    "\n",
    "    Now we get\n",
    "\n",
    "    -   Nvidia Quadro P1000: From 2.500 to 0.022 to 0.006 secs!\n",
    "\n",
    "        ``` bash\n",
    "        ==10662== NVPROF is profiling process 10662, command: ./a.out\n",
    "        Max error: 0\n",
    "        ==10662== Profiling application: ./a.out\n",
    "        ==10662== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  6.0868ms         1  6.0868ms  6.0868ms  6.0868ms  add(int, float*, float*)\n",
    "              API calls:   96.03%  165.28ms         2  82.641ms  13.911us  165.27ms  cudaMallocManaged\n",
    "                            3.54%  6.0887ms         1  6.0887ms  6.0887ms  6.0887ms  cudaDeviceSynchronize\n",
    "                            0.27%  460.56us         2  230.28us  184.71us  275.85us  cudaFree\n",
    "                            0.13%  215.37us       101  2.1320us     133ns  151.55us  cuDeviceGetAttribute\n",
    "                            0.02%  30.822us         1  30.822us  30.822us  30.822us  cudaLaunchKernel\n",
    "                            0.01%  22.122us         1  22.122us  22.122us  22.122us  cuDeviceGetName\n",
    "                            0.00%  5.7430us         1  5.7430us  5.7430us  5.7430us  cuDeviceGetPCIBusId\n",
    "                            0.00%  1.3810us         3     460ns     203ns     945ns  cuDeviceGetCount\n",
    "                            0.00%     921ns         2     460ns     163ns     758ns  cuDeviceGet\n",
    "                            0.00%     438ns         1     438ns     438ns     438ns  cuDeviceTotalMem\n",
    "                            0.00%     234ns         1     234ns     234ns     234ns  cuDeviceGetUuid\n",
    "\n",
    "        ==10662== Unified Memory profiling result:\n",
    "        Device \"Quadro P1000 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "              59  138.85KB  4.0000KB  0.9961MB  8.000000MB  740.3880us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  337.8280us  Device To Host\n",
    "              32         -         -         -           -  2.253582ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "    -   Testla T4: From 0.108 to 0.004 to 0.003 secs\n",
    "\n",
    "        ``` bash\n",
    "        ==8972== NVPROF is profiling process 8972, command: ./cuda_04.x\n",
    "        Max error: 0\n",
    "        ==8972== Profiling application: ./cuda_04.x\n",
    "        ==8972== Profiling result:\n",
    "                    Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n",
    "         GPU activities:  100.00%  2.9741ms         1  2.9741ms  2.9741ms  2.9741ms  add(int, float*, float*)\n",
    "              API calls:   98.47%  250.63ms         2  125.31ms  38.785us  250.59ms  cudaMallocManaged\n",
    "                            1.18%  2.9959ms         1  2.9959ms  2.9959ms  2.9959ms  cudaDeviceSynchronize\n",
    "                            0.24%  613.16us         2  306.58us  302.27us  310.89us  cudaFree\n",
    "                            0.07%  188.26us       101  1.8630us     169ns  86.068us  cuDeviceGetAttribute\n",
    "                            0.02%  38.874us         1  38.874us  38.874us  38.874us  cuDeviceGetName\n",
    "                            0.01%  37.051us         1  37.051us  37.051us  37.051us  cudaLaunchKernel\n",
    "                            0.00%  5.7050us         1  5.7050us  5.7050us  5.7050us  cuDeviceGetPCIBusId\n",
    "                            0.00%  2.2980us         3     766ns     224ns  1.8050us  cuDeviceGetCount\n",
    "                            0.00%     979ns         2     489ns     195ns     784ns  cuDeviceGet\n",
    "                            0.00%     587ns         1     587ns     587ns     587ns  cuDeviceTotalMem\n",
    "                            0.00%     367ns         1     367ns     367ns     367ns  cuModuleGetLoadingMode\n",
    "                            0.00%     324ns         1     324ns     324ns     324ns  cuDeviceGetUuid\n",
    "\n",
    "        ==8972== Unified Memory profiling result:\n",
    "        Device \"Tesla T4 (0)\"\n",
    "           Count  Avg Size  Min Size  Max Size  Total Size  Total Time  Name\n",
    "             106  77.282KB  4.0000KB  980.00KB  8.000000MB  969.6510us  Host To Device\n",
    "              24  170.67KB  4.0000KB  0.9961MB  4.000000MB  363.6760us  Device To Host\n",
    "              11         -         -         -           -  2.908132ms  Gpu page fault groups\n",
    "        Total CPU Page faults: 36\n",
    "        ```\n",
    "\n",
    "### <span class=\"todo TODO\">TODO</span> Openmp offload to gpu\n",
    "\n",
    "REF:\n",
    "\n",
    "-   <https://www.youtube.com/watch?v=uVcvecgdW7g>\n",
    "\n",
    "Code\n",
    "\n",
    "``` cpp\n",
    "#include <iostream>\n",
    "#include <cstdio>\n",
    "#include <omp.h>\n",
    "\n",
    "int main()\n",
    "{\n",
    "    int a[100], b[100], c[100];\n",
    "    int i;\n",
    "\n",
    "    // Initialize arrays a and b\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        a[i] = i;\n",
    "        b[i] = 2 * i;\n",
    "    }\n",
    "\n",
    "    int num_devices = omp_get_num_devices();\n",
    "    printf(\"Number of available devices %d\\n\", num_devices);\n",
    "\n",
    "    // Offload computation to GPU\n",
    "    #pragma omp target teams distribute parallel for map(to:a[0:100], b[0:100]) map(from:c[0:100])\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        c[i] = a[i] + b[i];\n",
    "    }\n",
    "\n",
    "    // Print results\n",
    "    for (i = 0; i < 100; i++) {\n",
    "        std::cout << c[i] << \" \";\n",
    "    }\n",
    "    std::cout << std::endl;\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Does not work in sala2 due to the error\n",
    "\n",
    "```bash\n",
    "OpenMP GPU Offload is available only on systems with NVIDIA GPUs with compute capability >= cc70\n",
    "```\n",
    "\n",
    "It seems that sala2 compute capability is 6.1. It can be get with\n",
    "\n",
    "``` bash\n",
    "nvidia-smi --query-gpu=compute_cap --format=csv\n",
    "```\n",
    "\n",
    "Using google collab I can compile it\n",
    "\n",
    "``` bash\n",
    "!nvcc -arch sm_75 -O3 -o openmp_offload openmp_offload.cpp -lgomp\n",
    "```\n",
    "\n",
    "and get\n",
    "\n",
    "``` bash\n",
    "Number of available devices 1\n",
    "0 3 6 9 12 15 18 21 24 27 30 33 36 ...\n",
    "```\n",
    "\n",
    "Check:\n",
    "\n",
    "-   <https://enccs.github.io/openmp-gpu/target/>\n",
    "\n",
    "``` c\n",
    "/* Copyright (c) 2019 CSC Training */\n",
    "/* Copyright (c) 2021 ENCCS */\n",
    "#include <stdio.h>\n",
    "\n",
    "#ifdef _OPENMP\n",
    "#include <omp.h>\n",
    "#endif\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int num_devices = omp_get_num_devices();\n",
    "  printf(\"Number of available devices %d\\n\", num_devices);\n",
    "\n",
    "  #pragma omp target\n",
    "  {\n",
    "      if (omp_is_initial_device()) {\n",
    "        printf(\"Running on host\\n\");\n",
    "      } else {\n",
    "        int nteams= omp_get_num_teams();\n",
    "        int nthreads= omp_get_num_threads();\n",
    "        printf(\"Running on device with %d teams in total and %d threads in each team\\n\",nteams,nthreads);\n",
    "      }\n",
    "  }\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3384c1-d9ae-4ac7-9b26-821659c60a25",
   "metadata": {},
   "source": [
    "\n",
    "### <span class=\"todo TODO\">TODO</span> OpenACC intro\n",
    "\n",
    "REF:\n",
    "\n",
    "-   <https://www.openacc.org/>\n",
    "-   <https://enccs.github.io/OpenACC-CUDA-beginners/1.02_openacc-introduction/>\n",
    "-   <https://ulhpc-tutorials.readthedocs.io/en/latest/gpu/openacc/basics/>\n",
    "\n",
    "Check if we are using the gpu or the cpu:\n",
    "\n",
    "``` c\n",
    "#include <stdio.h>\n",
    "#include <openacc.h>\n",
    "\n",
    "int main() {\n",
    "  int device_type = acc_get_device_type();\n",
    "\n",
    "  if (device_type == acc_device_nvidia) {\n",
    "    printf(\"Running on an NVIDIA GPU\\n\");\n",
    "  } else if (device_type == acc_device_radeon) {\n",
    "    printf(\"Running on an AMD GPU\\n\");\n",
    "    //} else if (device_type == acc_device_intel_mic) {\n",
    "    //printf(\"Running on an Intel MIC\\n\");\n",
    "  } else if (device_type == acc_device_host) {\n",
    "    printf(\"Running on the host CPU\\n\");\n",
    "  } else {\n",
    "    printf(\"Unknown device type\\n\");\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Compile as\n",
    "\n",
    "``` bash\n",
    "gcc -fopenacc mycode.c\n",
    "```\n",
    "\n",
    "Simple example:\n",
    "\n",
    "``` c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main() {\n",
    "  int i;\n",
    "  float a[100], b[100], c[100];\n",
    "\n",
    "  // Initialize arrays\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    a[i] = i;\n",
    "    b[i] = i;\n",
    "  }\n",
    "\n",
    "  // Compute element-wise sum\n",
    "  #pragma acc parallel loop\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    c[i] = a[i] + b[i];\n",
    "  }\n",
    "\n",
    "  // Print result\n",
    "  for (i = 0; i < 100; i++) {\n",
    "    printf(\"%f\\n\", c[i]);\n",
    "  }\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55b28f-fe04-40b0-960e-6cbaa02036b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a75ede6d-7c9d-48fa-bdb6-b55f0d9ffcc2",
   "metadata": {},
   "source": [
    "# Introduction to OpenMp (Shared memory)\n",
    "\n",
    "[OpenMp](http://www.openmp.org/) is an extension for languages like C/C++. It is targeted for\n",
    "**multi-threaded, shared memory** parallelism. It is very simple, you\n",
    "just add some pragma directives which, if not supported, are deactivated\n",
    "and then completely ignored. It is ideal for a machine with several\n",
    "cores, and shared memory. For much more info, see\n",
    "<http://www.openmp.org/> and <https://hpc-tutorials.llnl.gov/openmp/>. For a discussion of a recent release, check <https://news.ycombinator.com/item?id=42139843>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d823e-ce3b-4f62-8f1b-b66c1d423588",
   "metadata": {},
   "source": [
    "## Introduction: Hello world and threadid\n",
    "\n",
    "Typical Hello world,\n",
    "\n",
    "```c++\n",
    "#include <cstdio>\n",
    "#include \"omp.h\"\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "    double x = 9.0;\n",
    "//#pragma omp parallel num_threads(4)\n",
    "#pragma omp parallel //  This starts a parallel region\n",
    "    {// se generan los threads\n",
    "        std::printf(\"Hello, world.\\n\");\n",
    "        std::printf(\"Hello, world2.\\n\");\n",
    "    } // mueren los threads\n",
    "\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "This makes a parallel version if you compile with the `-fopemp` flag\n",
    "```sh\n",
    "g++ -fopenmp -g -fsanitize=thread code.cpp\n",
    "```\n",
    "\n",
    "Test it. What happens if you write\n",
    "\n",
    "```bash\n",
    "export OMP_NUM_THREADS=8\n",
    "```\n",
    "\n",
    "and then you run the executable?\n",
    "\n",
    "The actual directive is like\n",
    "\n",
    "```c++\n",
    "#pragma omp parallel [clause ...]  newline \n",
    "                   if (scalar_expression) \n",
    "                   private (list) \n",
    "                   shared (list) \n",
    "                   default (shared | none) \n",
    "                   firstprivate (list) \n",
    "                   reduction (operator: list) \n",
    "                   copyin (list) \n",
    "                   num_threads (integer-expression)\n",
    " structured_block  \n",
    "```\n",
    "\n",
    "But we will keep things simple.\n",
    "\n",
    "What is really important to keep in mind is what variables are going to\n",
    "be shared and what are going to be private, to avoid errors, reca\n",
    "conditions, etc.\n",
    "\n",
    "If you want to know the thread id, you can use\n",
    "\n",
    "```c++\n",
    "int nth =  omp_get_num_threads();\n",
    "int tid =  omp_get_thread_num();\n",
    "```\n",
    "\n",
    "inside your code, as in the following,\n",
    "\n",
    "```c++\n",
    "#include <cstdio> // printf\n",
    "#include <omp.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  double x = 9.0;\n",
    "\n",
    "  int nth = omp_get_num_threads();\n",
    "  int thid = omp_get_thread_num();\n",
    "  std::printf(\"Hello world from thid: %d, out of %d .\\n\",\n",
    "              thid, nth);\n",
    "\n",
    "//#pragma omp parallel num_threads(4)\n",
    "#pragma omp parallel\n",
    "  {// se generan los threads\n",
    "    int nth = omp_get_num_threads(); // al declarar aca, son privados\n",
    "    int thid = omp_get_thread_num();\n",
    "    std::printf(\"Hello world from thid: %d, out of %d .\\n\",\n",
    "                thid, nth);\n",
    "  } // mueren los threads\n",
    "\n",
    "  std::printf(\"Hello world from thid: %d, out of %d .\\n\",\n",
    "              thid, nth);\n",
    "\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Please use the thread sanitize:\n",
    "\n",
    "```sh\n",
    "g++ -fopenmp -g -fsanitize=thread code.cpp\n",
    "```\n",
    "\n",
    "There are some other env variables that could be useful:\n",
    "\n",
    "```sh\n",
    "OMP_NUM_THREADS=4\n",
    "OMP_DISPLAY_ENV=TRUE\n",
    "OMP_DISPLAY_AFFINITY=TRUE\n",
    "OMP_STACK_SIZE=1000000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e701b2-f0c4-484a-bb85-51fc6bd1d362",
   "metadata": {},
   "source": [
    "## Running openmp processes with slurm\n",
    "To run on the cluster, run as\n",
    "```bash\n",
    "export OMP_NUM_THREADS=8  # Request 8 threads for your OpenMP program\n",
    "srun --cpus-per-task=8 --pty ./my_openmp_program\n",
    "```\n",
    "\n",
    "Here the key is to use `--cpus-per-task=8`, which means use 8 threads per process (task). You can even specify everythin in the command\n",
    "```bash\n",
    "srun --cpus-per-task=8 --pty bash -c \"export OMP_NUM_THREADS=8; ./my_openmp_program\"\n",
    "```\n",
    "\n",
    "or , for better consistency and avoiding duplications, asking slurm for the reserved threads\n",
    "\n",
    "```bash\n",
    "srun --cpus-per-task=4 --time=00:01:00 --pty bash -c \"export OMP_NUM_THREADS=\\$SLURM_CPUS_PER_TASK; ./my_openmp_program\"\n",
    "```\n",
    "\n",
    "Of course, it is much better to use a script,\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --job-name=openmp_job      # Job name\n",
    "#SBATCH --output=openmp_job_%j.out # Standard output and error log\n",
    "#SBATCH --error=openmp_job_%j.err  # %j is replaced with the job ID\n",
    "#SBATCH --nodes=1                  # Request 1 node\n",
    "#SBATCH --ntasks=1                 # Request 1 task (your OpenMP program is a single task)\n",
    "#SBATCH --cpus-per-task=8          # Request 8 CPUs for this task (adjust as needed for your program)\n",
    "#SBATCH --time=00:10:00            # Wall-clock time limit (HH:MM:SS)\n",
    "#SBATCH --mem=4G                   # Memory per node (adjust as needed)\n",
    "\n",
    "# Load any necessary modules (e.g., GCC if it's not in your default path)\n",
    "# module load gcc/11.2.0 # Example\n",
    "\n",
    "# Set the number of OpenMP threads\n",
    "# This should ideally be equal to --cpus-per-task for optimal performance\n",
    "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
    "\n",
    "echo \"Starting OpenMP job on node: $(hostname)\"\n",
    "echo \"Number of requested CPUs per task: $SLURM_CPUS_PER_TASK\"\n",
    "echo \"OMP_NUM_THREADS set to: $OMP_NUM_THREADS\"\n",
    "\n",
    "# Run your OpenMP program\n",
    "./a.out\n",
    "\n",
    "echo \"OpenMP job finished.\"\n",
    "```\n",
    " and then `sbatch`:\n",
    "```bash\n",
    "sbatch openmp.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d029a3",
   "metadata": {},
   "source": [
    "\n",
    "## Private and shared variables\n",
    "\n",
    "Shared and private\n",
    "\n",
    "```c++\n",
    "#include <cstdio>\n",
    "#include <omp.h>\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  double x = 9.0; // \"public\" var, seen and shared by all threads \n",
    "\n",
    "  int nth = omp_get_num_threads();\n",
    "  int thid = omp_get_thread_num();\n",
    "  std::printf(\"Hello world from thid: %d, out of %d .\\n\",\n",
    "              thid, nth);\n",
    "\n",
    "//#pragma omp parallel num_threads(4) // set the num_threads instead of relying on OMP_NUM_THREADS\n",
    "#pragma omp parallel private(thid, nth) // makes thid and nth private, not problem with overwriting\n",
    "  {// se generan los threads\n",
    "    thid = omp_get_thread_num(); // privada, aunque tenga el mismo nombre, tipo, y no haya sido redeclarada\n",
    "    nth = omp_get_num_threads(); // privada, aunque tenga el mismo nombre, tipo, y no haya sido redeclarada\n",
    "    std::printf(\"Hello world from thid: %d, out of %d .\\n\",\n",
    "                thid, nth);\n",
    "  } // mueren los threads\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "In this new example, we see the memory address for shared and private\n",
    "variables to illustrate those concepts:\n",
    "\n",
    "```c++\n",
    "#include <omp.h>\n",
    "#include <iostream>\n",
    "#include <cstdio>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  int nthreads, tid;\n",
    "\n",
    "  /* Fork a team of threads with each thread having a private tid variable */\n",
    "#pragma omp parallel private(tid)\n",
    "  {\n",
    "\n",
    "    /* Obtain and print thread id */\n",
    "    tid = omp_get_thread_num();\n",
    "    std::printf(\"Hello World from thread = %d\\n\", tid);\n",
    "    std::cout << \"Memory address for tid = \" << &tid << std::endl;\n",
    "    std::cout << \"Memory address for nthreads = \" << &nthreads << std::endl;\n",
    "\n",
    "    /* Only master thread does this */\n",
    "    if (tid == 0) \n",
    "      {\n",
    "        nthreads = omp_get_num_threads();\n",
    "        printf(\"Number of threads = %d\\n\", nthreads);\n",
    "      }\n",
    "\n",
    "  }  /* All threads join master thread and terminate */\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "```\n",
    "\n",
    "Knowing the thread id allows to distribute job. For instance, the following shows how to select code only for the master thread\n",
    "```c++\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  int nthreads, tid;\n",
    "\n",
    "  /* Fork a team of threads with each thread having a private tid variable */\n",
    "#pragma omp parallel private(tid)\n",
    "  {\n",
    "\n",
    "    /* Obtain and print thread id */\n",
    "    tid = omp_get_thread_num();\n",
    "    printf(\"Hello World from thread = %d\\n\", tid);\n",
    "\n",
    "    /* Only master thread does this */\n",
    "    if (tid == 0)\n",
    "    {\n",
    "      nthreads = omp_get_num_threads();\n",
    "      printf(\"Number of threads = %d\\n\", nthreads);\n",
    "    }\n",
    "\n",
    "  }  /* All threads join master thread and terminate */\n",
    "\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7bcb42",
   "metadata": {},
   "source": [
    "\n",
    "## Parallelizing a for loop\n",
    "\n",
    "We want to sum all elements in a vector, in parallel. If the vector is of size N, and we have nth threads, how to distribute the job? Each thread will process a local part of size `Nlocal = N/nth` (what happens if N is not divisible by nth?). Then, each thread needs to process each segment. For example, let's say that N = 1000000, and nth = 5. Then Nlocal  = 20000. Here is important to think in local and global indexes:\n",
    "- **local indexes**: Each thread will run from a local index 0 up to Nlocal-1. This index represents the local part being processed.\n",
    "- **global indexes**: This is the original index for the whole array. This goes from 0 to N. In our case, for thread 0 it will run globally from 0 up to Nlocal-1, then for thread 1 it will run from Nlocal up to 2Nlocal-1, and so on. In general, for a thread id tid we have a minium local index equal to `imin = tid*Nlocal`, and a maximum (not inclusive) equal to `imax = imin + Nlocal = (tid+1)*Nlocal`.\n",
    "\n",
    "It is crucial to compute this job partition among threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba00251",
   "metadata": {},
   "source": [
    "\n",
    "### Manually\n",
    "\n",
    "The following code assigns the elements of a vector in parallel:\n",
    "\n",
    "```c++\n",
    "#include <omp.h>\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "#include <vector>\n",
    "#include <cstdlib>\n",
    "#include <numeric>\n",
    "\n",
    "void fill(std::vector<double> & array);\n",
    "double suma(const std::vector<double> & array);\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  std::vector<double> data(N);\n",
    "\n",
    "  // llenar el arreglo con some datos\n",
    "  fill(data);\n",
    "  //std::cout << data[0] << \"\\n\";\n",
    "\n",
    "  // calcular la suma y el promedio\n",
    "  double total = suma(data);\n",
    "  std::cout << total/data.size() << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "void fill(std::vector<double> & array)\n",
    "{\n",
    "  int N = array.size();\n",
    "#pragma omp parallel\n",
    "  {\n",
    "    int thid = omp_get_thread_num();\n",
    "    int nth = omp_get_num_threads();\n",
    "    int Nlocal = N/nth;\n",
    "    int iimin = thid*Nlocal;\n",
    "    int iimax = iimin + Nlocal;\n",
    "    for(int ii = iimin; ii < iimax; ii++) {\n",
    "      array[ii] = 2*ii*std::sin(std::sqrt(ii/56.7)) +\n",
    "        std::cos(std::pow(1.0*ii*ii/N, 0.3));\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "double suma(const std::vector<double> & array)\n",
    "{\n",
    "  int N = array.size();\n",
    "  int max_threads_possible = omp_get_max_threads();\n",
    "  std::vector<double> sumaparcial(max_threads_possible, 0.0);\n",
    "\n",
    "#pragma omp parallel\n",
    "  {\n",
    "    int thid = omp_get_thread_num();\n",
    "    int nth = omp_get_num_threads();\n",
    "    double localsum = 0.0;\n",
    "    int Nlocal = N/nth;\n",
    "    int iimin = thid*Nlocal;\n",
    "    int iimax = iimin + Nlocal;\n",
    "    for(int ii = iimin; ii < iimax; ii++) {\n",
    "      localsum += array[ii];\n",
    "    }\n",
    "    sumaparcial[thid] = localsum; // save the localsum in a different place per thread\n",
    "  }\n",
    "  return std::accumulate(sumaparcial.begin(), sumaparcial.end(), 0.0); // sum up all partial sums\n",
    "}\n",
    "```\n",
    "\n",
    "Then, after compilation, \n",
    "```bash\n",
    "g++ -std=c++17 -g -fopenmp suma.cpp -o suma.x\n",
    "```\n",
    "\n",
    "you can run it as \n",
    "```bash\n",
    "srun --cpus-per-task=1 --time=00:01:00 bash -c \"export OMP_NUM_THREADS=\\$SLURM_CPUS_PER_TASK; ./suma.x 50000000\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dee8af",
   "metadata": {},
   "source": [
    "\n",
    "### Using `omp parallel for`\n",
    "Actually it is much easier to use a parallel for and a reduction to accumulate the data. With this, openmp will take care of all the work splitting. \n",
    "\n",
    "``` c++\n",
    "#include <omp.h>\n",
    "#include <iostream>\n",
    "#include <cmath>\n",
    "#include <vector>\n",
    "#include <cstdlib>\n",
    "#include <numeric>\n",
    "\n",
    "void fill(std::vector<double> & array);\n",
    "double suma(const std::vector<double> & array);\n",
    "\n",
    "int main(int argc, char *argv[])\n",
    "{\n",
    "  const int N = std::atoi(argv[1]);\n",
    "  std::vector<double> data(N);\n",
    "\n",
    "  // llenar el arreglo\n",
    "  fill(data);\n",
    "  //std::cout << data[0] << \"\\n\";\n",
    "\n",
    "  // calcular la suma y el promedio\n",
    "  double total = suma(data);\n",
    "  std::cout << total/data.size() << \"\\n\";\n",
    "\n",
    "  return 0;\n",
    "}\n",
    "\n",
    "void fill(std::vector<double> & array)\n",
    "{\n",
    "  const int N = array.size();\n",
    "#pragma omp parallel for\n",
    "  for(int ii = 0; ii < N; ii++) {\n",
    "      array[ii] = 2*ii*std::sin(std::sqrt(ii/56.7)) +\n",
    "        std::cos(std::pow(1.0*ii*ii/N, 0.3));\n",
    "  }\n",
    "}\n",
    "\n",
    "double suma(const std::vector<double> & array)\n",
    "{\n",
    "  int N = array.size();\n",
    "  double suma = 0.0;\n",
    "#pragma omp parallel for reduction(+:suma)\n",
    "  for(int ii = 0; ii < N; ii++) {\n",
    "    suma += array[ii];\n",
    "  }\n",
    "  return suma;\n",
    "}\n",
    "\n",
    "\n",
    "```\n",
    "Then, after compilation, \n",
    "```bash\n",
    "g++ -std=c++17 -g -fopenmp parallelfor.cpp -o parallelfor.x\n",
    "```\n",
    "\n",
    "you can run it as \n",
    "```bash\n",
    "srun --cpus-per-task=1 --time=00:01:00 bash -c \"export OMP_NUM_THREADS=\\$SLURM_CPUS_PER_TASK; ./parallelfor.x 50000000\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620b5b07",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "Compute the parallel metrics for the last code . Use `omp_get_wtime(void)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1869f",
   "metadata": {},
   "source": [
    "## Tutorial and extra materials:\n",
    "-   <https://researchcomputing.princeton.edu/education/external-online-resources/openmp>\n",
    "-   <https://cvw.cac.cornell.edu/openmp/default>\n",
    "-   Scheduling:\n",
    "    -   <https://610yilingliu.github.io/2020/07/15/ScheduleinOpenMP/>\n",
    "    -   <https://www.openmp.org/wp-content/uploads/SC17-Kale-LoopSchedforOMP_BoothTalk.pdf>\n",
    "    -   <http://jakascorner.com/blog/2016/06/omp-for-scheduling.html>\n",
    "\n",
    "Parallel STL\n",
    "- <https://www.modernescpp.com/index.php?option=com_content&view=article&id=572&catid=49>\n",
    "- <https://www.modernescpp.com/index.php?option=com_content&view=article&id=573&catid=49>\n",
    "\n",
    "\n",
    "> Please notice that recent openmp standards allow you to offload work to the gpu\n",
    "```c++\n",
    "#include <omp.h>\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define N 1000000\n",
    "\n",
    "int main() {\n",
    "    float *a = (float*) malloc(N * sizeof(float));\n",
    "    float *b = (float*) malloc(N * sizeof(float));\n",
    "    float *c = (float*) malloc(N * sizeof(float));\n",
    "\n",
    "    // Initialize input vectors\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        a[i] = i * 1.0f;\n",
    "        b[i] = i * 2.0f;\n",
    "    }\n",
    "\n",
    "    // Offload the work to the GPU\n",
    "    #pragma omp target data map(to: a[0:N], b[0:N]) map(from: c[0:N])\n",
    "    {\n",
    "        #pragma omp target teams distribute parallel for\n",
    "        for (int i = 0; i < N; i++) {\n",
    "            c[i] = a[i] + b[i];\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // Verify result\n",
    "    int errors = 0;\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        if (c[i] != a[i] + b[i]) {\n",
    "            errors++;\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if (errors == 0)\n",
    "        printf(\"Success! All values correct.\\n\");\n",
    "    else\n",
    "        printf(\"There were %d errors.\\n\", errors);\n",
    "\n",
    "    free(a); free(b); free(c);\n",
    "    return 0;\n",
    "}\n",
    "\n",
    "```\n",
    "and then compile as \n",
    "```bash\n",
    "g++ -fopenmp -foffload=nvptx-none -O2 -o vec_add vec_add.c # you can also offload to amd, using amd compiler\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f369569",
   "metadata": {},
   "source": [
    "\n",
    "## Exercises\n",
    "\n",
    "Refs: CT-LAB openmp exercises;\n",
    "<https://computing.llnl.gov/tutorials/openMP/exercise.html>\n",
    "\n",
    "1.  Write a program to compute the mean and the standard deviation for a\n",
    "    large array using OpenMP, with only one reduction. Also compute the\n",
    "    parallel metrics.\n",
    "\n",
    "    ```c++\n",
    "    // stats.cpp\n",
    "    #include <omp.h>\n",
    "    #include <iostream>\n",
    "    #include <cmath>\n",
    "    #include <vector>\n",
    "    #include <cstdlib>\n",
    "    #include <numeric>\n",
    "\n",
    "    void fill(std::vector<double> & array);\n",
    "    void stats(const std::vector<double> & array, double &mean, double &sigma);\n",
    "\n",
    "    int main(int argc, char *argv[])\n",
    "    {\n",
    "      const int N = std::atoi(argv[1]);\n",
    "      std::vector<double> data(N);\n",
    "\n",
    "      // llenar el arreglo\n",
    "      fill(data);\n",
    "\n",
    "      // calcular stats\n",
    "      double mean{0.0}, sigma{0.0};\n",
    "      double start = omp_get_wtime();\n",
    "      stats(data, mean, sigma);\n",
    "      double time = omp_get_wtime() - start;\n",
    "      std::printf(\"%.15le\\t\\t%.15le\\t\\t%.15le\\n\", mean, sigma, time);\n",
    "\n",
    "      return 0;\n",
    "    }\n",
    "\n",
    "    void fill(std::vector<double> & array)\n",
    "    {\n",
    "      const int N = array.size();\n",
    "    #pragma omp parallel for\n",
    "      for(int ii = 0; ii < N; ii++) {\n",
    "          array[ii] = 2*ii*std::sin(std::sqrt(ii/56.7)) +\n",
    "            std::cos(std::pow(1.0*ii*ii/N, 0.3));\n",
    "      }\n",
    "    }\n",
    "\n",
    "    void stats(const std::vector<double> & array, double & mean, double & sigma)\n",
    "    {\n",
    "      int N = array.size();\n",
    "      double suma = 0.0;\n",
    "    #pragma omp parallel for reduction(+:suma)\n",
    "      for(int ii = 0; ii < N; ii++) {\n",
    "        suma += array[ii];\n",
    "      }\n",
    "      mean = suma/N;\n",
    "    }\n",
    "\n",
    "    ```\n",
    "\n",
    "2.  Write a program to compute the integral of the function $y =\n",
    "      x^{2}$, for \\$ x  ∈ \\[0,10\\]\\$. Fill the following table:\n",
    "\n",
    "    | \\# Threads | Runtime \\[s\\] | Speedup | Efficiency |\n",
    "    |------------|---------------|---------|------------|\n",
    "\n",
    "    Explain your results in terms of the number of processors available.\n",
    "    Use $N=12000$. Do it in two ways:\n",
    "\n",
    "    1.  Distributing the N intervals across all threads, so each one has\n",
    "        a smaller part.\n",
    "    2.  Keeping constant the resolution per thread, that is , each\n",
    "        trhead has N intervals.\n",
    "\n",
    "    What is the difference between the two cases, regarding the\n",
    "    precision and the time?\n",
    "\n",
    "```c++\n",
    "#include <iostream>\n",
    "#include <omp.h>\n",
    "\n",
    "using fptr = double(double);\n",
    "\n",
    "double f(double x);\n",
    "double integral_serial(double a, double b, int N, fptr f);\n",
    "double integral_openmp(double a, double b, int N, fptr f);\n",
    "\n",
    "int main(void)\n",
    "{\n",
    "  // declare vars\n",
    "  const double XA = 0.0; \n",
    "  const double XB = 10.0; \n",
    "  const int N = 100000000;\n",
    "\n",
    "  // print result\n",
    "  //std::cout << \"Serial integral: \" << integral_serial(XA, XB, N, f) << \"\\n\";\n",
    "  //std::cout << \"Serial openmp  : \" << integral_openmp(XA, XB, N, f) << \"\\n\";\n",
    "  double t1 = omp_get_wtime();\n",
    "  integral_openmp(XA, XB, N, f);\n",
    "  double t2 = omp_get_wtime();\n",
    "\n",
    "#pragma omp parallel\n",
    "  {\n",
    "    if(0 == omp_get_thread_num()) {\n",
    "      std::cout << omp_get_num_threads() << \"\\t\" << t2 - t1 << std::endl;\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "double f(double x)\n",
    "{\n",
    "  return x*x;\n",
    "}\n",
    "\n",
    "double integral_serial(double a, double b, int N, fptr f)\n",
    "{\n",
    "  const double dx = (b-a)/N; \n",
    "  // compute integral\n",
    "  double sum = 0, x;\n",
    "  for(int ii = 0; ii < N; ++ii) {\n",
    "    x = a + ii*dx;\n",
    "    sum += dx*f(x);\n",
    "  }\n",
    "  return sum;\n",
    "}\n",
    "\n",
    "double integral_openmp(double a, double b, int N, fptr f)\n",
    "{\n",
    "  TODO\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "1.  Parallelize a Matrix-Matrix multiplication. Compare the performance\n",
    "    when you use one, two, three, for threads.\n",
    "\n",
    "2.  Is this loop parallelizable? If not, why?\n",
    "\n",
    "    ```bash\n",
    "    #pragma omp parallel for\n",
    "    for (int i = 1; i < N; i++)\n",
    "    {\n",
    "      A[i] = B[i] – A[i – 1];\n",
    "    }\n",
    "    ```\n",
    "\n",
    "3.  Parallelize a matrix-vector multiplication. What must be shared?\n",
    "    what should be private?\n",
    "\n",
    "4.  Parallelize the matrix transposition.\n",
    "\n",
    "5.  Check and solve the exercises on\n",
    "    <https://computing.llnl.gov/tutorials/openMP/exercise.html> .\n",
    "\n",
    "6.  Check and solve <http://www.hpc.cineca.it/content/training-openmp> .\n",
    "\n",
    "7.  Some other examples are at\n",
    "    <https://www.archer.ac.uk/training/course-material/2015/03/thread_prog/>\n",
    "    ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3d70b-9bb1-4ab9-abed-2fcd0ffb9766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
